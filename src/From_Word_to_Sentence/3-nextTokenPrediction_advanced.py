#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Next Token Prediction è¿›é˜¶æ¼”ç¤º
ç»“åˆç®€å•æ¨¡å‹å’Œç†è®ºåˆ†æï¼Œæ·±å…¥ç†è§£Next Token PredictionåŸç†

Author: AI Algorithm Course  
Date: 2025-09-11
"""

import json
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
from typing import List, Dict, Tuple, Optional
import re
from collections import Counter, defaultdict
import pickle
from tqdm import tqdm
import math

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class AdvancedTokenizer:
    """è¿›é˜¶åˆ†è¯å™¨ï¼Œæ”¯æŒè¯çº§å’Œå­—ç¬¦çº§åˆ†è¯"""
    
    def __init__(self, vocab_size: int = 5000):
        self.vocab_size = vocab_size
        self.word_to_id = {}
        self.id_to_word = {}
        self.char_to_id = {}
        self.id_to_char = {}
        self.word_freq = Counter()
        self.char_freq = Counter()
        
    def build_vocab(self, texts: List[str], tokenization_level: str = "mixed"):
        """
        æ„å»ºè¯æ±‡è¡¨
        
        Args:
            texts: è®­ç»ƒæ–‡æœ¬
            tokenization_level: "word", "char", "mixed"
        """
        print(f"æ„å»ºè¯æ±‡è¡¨ (çº§åˆ«: {tokenization_level})...")
        
        # æ”¶é›†è¯å’Œå­—ç¬¦é¢‘ç‡
        all_words = []
        all_chars = set()
        
        for text in texts:
            # ç®€å•çš„ä¸­æ–‡åˆ†è¯ï¼ˆæŒ‰æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼åˆ†å‰²ï¼‰
            words = re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]+|\d+|[^\s\u4e00-\u9fff]', text)
            all_words.extend(words)
            
            # å­—ç¬¦çº§åˆ«
            chars = list(text)
            all_chars.update(chars)
        
        self.word_freq = Counter(all_words)
        self.char_freq = Counter(all_chars)
        
        # æ„å»ºè¯çº§åˆ«è¯æ±‡è¡¨
        special_tokens = ['<PAD>', '<UNK>', '<BOS>', '<EOS>']
        most_common_words = [word for word, _ in self.word_freq.most_common(self.vocab_size - len(special_tokens))]
        word_vocab = special_tokens + most_common_words
        
        self.word_to_id = {word: i for i, word in enumerate(word_vocab)}
        self.id_to_word = {i: word for i, word in enumerate(word_vocab)}
        
        # æ„å»ºå­—ç¬¦çº§åˆ«è¯æ±‡è¡¨
        char_vocab = special_tokens + sorted(list(all_chars))
        self.char_to_id = {char: i for i, char in enumerate(char_vocab)}
        self.id_to_char = {i: char for i, char in enumerate(char_vocab)}
        
        print(f"è¯çº§åˆ«è¯æ±‡è¡¨å¤§å°: {len(self.word_to_id)}")
        print(f"å­—ç¬¦çº§åˆ«è¯æ±‡è¡¨å¤§å°: {len(self.char_to_id)}")
        
    def tokenize_text(self, text: str, level: str = "word") -> List[str]:
        """åˆ†è¯"""
        if level == "word":
            return re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]+|\d+|[^\s\u4e00-\u9fff]', text)
        elif level == "char":
            return list(text)
        else:
            raise ValueError("level must be 'word' or 'char'")
    
    def encode(self, text: str, level: str = "word", max_length: Optional[int] = None) -> List[int]:
        """ç¼–ç æ–‡æœ¬"""
        tokens = self.tokenize_text(text, level)
        
        if level == "word":
            ids = [self.word_to_id.get(token, self.word_to_id['<UNK>']) for token in tokens]
        else:
            ids = [self.char_to_id.get(token, self.char_to_id['<UNK>']) for token in tokens]
        
        if max_length:
            if len(ids) > max_length:
                ids = ids[:max_length]
            else:
                pad_id = self.word_to_id['<PAD>'] if level == "word" else self.char_to_id['<PAD>']
                ids.extend([pad_id] * (max_length - len(ids)))
        
        return ids
    
    def decode(self, ids: List[int], level: str = "word") -> str:
        """è§£ç IDåºåˆ—"""
        if level == "word":
            tokens = [self.id_to_word.get(id, '<UNK>') for id in ids]
            # è¿‡æ»¤ç‰¹æ®Štokenå¹¶è¿æ¥
            tokens = [token for token in tokens if token not in ['<PAD>', '<UNK>', '<BOS>', '<EOS>']]
            return ''.join(tokens)
        else:
            chars = [self.id_to_char.get(id, '<UNK>') for id in ids]
            chars = [char for char in chars if char not in ['<PAD>', '<UNK>', '<BOS>', '<EOS>']]
            return ''.join(chars)

class TransformerLanguageModel(nn.Module):
    """åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹"""
    
    def __init__(self, vocab_size: int, d_model: int = 256, nhead: int = 8, 
                 num_layers: int = 4, max_seq_len: int = 512):
        super().__init__()
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.max_seq_len = max_seq_len
        
        # è¯åµŒå…¥å’Œä½ç½®ç¼–ç 
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = self._create_positional_encoding(max_seq_len, d_model)
        
        # ä½¿ç”¨Encoderå±‚ï¼Œä½†æ·»åŠ å› æœæ©ç æ¨¡æ‹ŸDecoderè¡Œä¸º
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,
            dropout=0.1,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
        
        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Linear(d_model, vocab_size)
        self.dropout = nn.Dropout(0.1)
        
        # åˆå§‹åŒ–å‚æ•°
        self._init_weights()
    
    def _create_positional_encoding(self, max_len: int, d_model: int) -> torch.Tensor:
        """åˆ›å»ºä½ç½®ç¼–ç """
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        return pe.unsqueeze(0)  # [1, max_len, d_model]
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
    
    def _generate_square_subsequent_mask(self, size: int) -> torch.Tensor:
        """ç”Ÿæˆå› æœæ³¨æ„åŠ›æ©ç """
        mask = torch.triu(torch.ones(size, size), diagonal=1)
        mask = mask.masked_fill(mask == 1, float('-inf'))
        return mask
    
    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None):
        """å‰å‘ä¼ æ’­"""
        batch_size, seq_len = input_ids.shape
        
        # è¯åµŒå…¥
        embeddings = self.embedding(input_ids) * math.sqrt(self.d_model)
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        pos_encoding = self.pos_encoding[:, :seq_len, :].to(input_ids.device)
        embeddings = embeddings + pos_encoding
        embeddings = self.dropout(embeddings)
        
        # ç”Ÿæˆå› æœæ©ç 
        src_mask = self._generate_square_subsequent_mask(seq_len).to(input_ids.device)
        
        # Transformer Encoder with causal mask
        output = self.transformer(embeddings, mask=src_mask)
        
        # è¾“å‡ºæŠ•å½±
        logits = self.output_proj(output)
        
        return logits
    
    def predict_next_token_with_analysis(self, input_ids: torch.Tensor, 
                                       temperature: float = 1.0, 
                                       top_k: int = 10) -> Dict:
        """é¢„æµ‹ä¸‹ä¸€ä¸ªtokenå¹¶æä¾›è¯¦ç»†åˆ†æ"""
        self.eval()
        with torch.no_grad():
            # å‰å‘ä¼ æ’­
            logits = self.forward(input_ids)
            last_logits = logits[0, -1, :] / temperature
            
            # è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            attention_analysis = self._analyze_attention(input_ids)
            
            # åº”ç”¨softmax
            probs = F.softmax(last_logits, dim=-1)
            
            # è·å–top-k
            top_probs, top_indices = torch.topk(probs, top_k)
            
            # è®¡ç®—å›°æƒ‘åº¦
            entropy = -torch.sum(probs * torch.log(probs + 1e-10))
            perplexity = torch.exp(entropy)
            
            return {
                "top_tokens": list(zip(top_indices.cpu().numpy(), top_probs.cpu().numpy())),
                "entropy": entropy.item(),
                "perplexity": perplexity.item(),
                "attention_analysis": attention_analysis,
                "full_probs": probs.cpu().numpy()
            }
    
    def _analyze_attention(self, input_ids: torch.Tensor) -> Dict:
        """åˆ†ææ³¨æ„åŠ›æ¨¡å¼ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        # è¿™é‡Œæä¾›ä¸€ä¸ªç®€åŒ–çš„æ³¨æ„åŠ›åˆ†æ
        seq_len = input_ids.shape[1]
        
        # æ¨¡æ‹Ÿæ³¨æ„åŠ›æƒé‡åˆ†å¸ƒ
        attention_weights = torch.softmax(torch.randn(seq_len, seq_len), dim=-1)
        
        return {
            "attention_weights": attention_weights.cpu().numpy(),
            "max_attention_position": torch.argmax(attention_weights[-1, :]).item()
        }

class NextTokenPredictionLab:
    """Next Token Prediction å®éªŒå®¤"""
    
    def __init__(self, data_file: str):
        self.data_file = data_file
        self.tokenizer = AdvancedTokenizer()
        self.model = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.training_stats = {
            "losses": [], "perplexities": [], "learning_rates": []
        }
        
        print(f"ğŸ”¬ Next Token Prediction å®éªŒå®¤")
        print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def load_and_prepare_data(self):
        """åŠ è½½å¹¶å‡†å¤‡æ•°æ®"""
        with open(self.data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
        texts = []
        texts.extend(data.get("prompts", []))
        texts.extend(data.get("contexts", []))
        
        # æ„å»ºè¯æ±‡è¡¨
        self.tokenizer.build_vocab(texts, tokenization_level="mixed")
        
        print(f"ğŸ“š åŠ è½½äº† {len(texts)} ä¸ªæ–‡æœ¬æ ·æœ¬")
        return texts
    
    def create_model(self, model_type: str = "transformer"):
        """åˆ›å»ºæ¨¡å‹"""
        if model_type == "transformer":
            self.model = TransformerLanguageModel(
                vocab_size=len(self.tokenizer.word_to_id),
                d_model=256,
                nhead=8,
                num_layers=4,
                max_seq_len=128
            ).to(self.device)
        else:
            raise ValueError(f"Unsupported model type: {model_type}")
        
        num_params = sum(p.numel() for p in self.model.parameters())
        print(f"ğŸ¤– åˆ›å»º {model_type} æ¨¡å‹ï¼Œå‚æ•°é‡: {num_params:,}")
    
    def train_model(self, texts: List[str], epochs: int = 20, batch_size: int = 16):
        """è®­ç»ƒæ¨¡å‹"""
        print("\nğŸ‹ï¸ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = self._create_dataset(texts, seq_len=16)  # å‡å°åºåˆ—é•¿åº¦
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        # è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-4, weight_decay=0.01)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
        criterion = nn.CrossEntropyLoss(ignore_index=self.tokenizer.word_to_id['<PAD>'])
        
        self.model.train()
        for epoch in range(epochs):
            total_loss = 0
            num_batches = 0
            
            progress_bar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}")
            for batch_inputs, batch_targets in progress_bar:
                batch_inputs = batch_inputs.to(self.device)
                batch_targets = batch_targets.to(self.device)
                
                optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                logits = self.model(batch_inputs)
                
                # è®¡ç®—æŸå¤±
                loss = criterion(logits.reshape(-1, self.model.vocab_size), 
                               batch_targets.reshape(-1))
                
                # åå‘ä¼ æ’­
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()
                
                total_loss += loss.item()
                num_batches += 1
                
                # æ›´æ–°è¿›åº¦æ¡
                progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})
            
            # è®¡ç®—å¹³å‡æŸå¤±å’Œå›°æƒ‘åº¦
            avg_loss = total_loss / num_batches
            perplexity = math.exp(avg_loss)
            current_lr = scheduler.get_last_lr()[0]
            
            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            self.training_stats["losses"].append(avg_loss)
            self.training_stats["perplexities"].append(perplexity)
            self.training_stats["learning_rates"].append(current_lr)
            
            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step()
            
            if (epoch + 1) % 5 == 0:
                print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, "
                      f"Perplexity: {perplexity:.2f}, LR: {current_lr:.6f}")
        
        print("âœ… è®­ç»ƒå®Œæˆ!")
    
    def _create_dataset(self, texts: List[str], seq_len: int):
        """åˆ›å»ºæ•°æ®é›†"""
        class TextDataset(Dataset):
            def __init__(self, texts, tokenizer, seq_len):
                self.data = []
                
                # åˆå¹¶æ‰€æœ‰æ–‡æœ¬åˆ›å»ºæ›´é•¿çš„åºåˆ—
                combined_text = " ".join(texts)
                tokens = tokenizer.encode(combined_text, level="word")
                
                print(f"æ€»tokenæ•°é‡: {len(tokens)}, åºåˆ—é•¿åº¦: {seq_len}")
                
                # å¦‚æœtokenså¤ªå°‘ï¼Œè‡³å°‘åˆ›å»ºä¸€äº›æ ·æœ¬
                if len(tokens) < seq_len + 1:
                    print("âš ï¸ æ–‡æœ¬å¤ªçŸ­ï¼Œé‡å¤æ–‡æœ¬ä»¥åˆ›å»ºè¶³å¤Ÿçš„è®­ç»ƒæ•°æ®")
                    # é‡å¤æ–‡æœ¬ç›´åˆ°æœ‰è¶³å¤Ÿçš„tokens
                    while len(tokens) < seq_len + 10:
                        tokens.extend(tokenizer.encode(combined_text, level="word"))
                
                # åˆ›å»ºè®­ç»ƒåºåˆ—
                for i in range(len(tokens) - seq_len):
                    input_seq = tokens[i:i+seq_len]
                    target_seq = tokens[i+1:i+seq_len+1]
                    self.data.append((input_seq, target_seq))
                
                print(f"åˆ›å»ºäº† {len(self.data)} ä¸ªè®­ç»ƒæ ·æœ¬")
            
            def __len__(self):
                return len(self.data)
            
            def __getitem__(self, idx):
                input_seq, target_seq = self.data[idx]
                return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)
        
        return TextDataset(texts, self.tokenizer, seq_len)
    
    def comprehensive_analysis(self, test_texts: List[str], save_dir: str):
        """ç»¼åˆåˆ†æNext Token Prediction"""
        print("\nğŸ” è¿›è¡Œç»¼åˆåˆ†æ...")
        
        results = {
            "token_predictions": [],
            "generation_analysis": [],
            "probability_distributions": [],
            "attention_patterns": []
        }
        
        for i, text in enumerate(test_texts[:3]):
            print(f"\nåˆ†ææ–‡æœ¬ {i+1}: {text[:50]}...")
            
            # ç¼–ç è¾“å…¥
            input_tokens = self.tokenizer.encode(text[:30], level="word")
            input_ids = torch.tensor([input_tokens]).to(self.device)
            
            # è¯¦ç»†é¢„æµ‹åˆ†æ
            analysis = self.model.predict_next_token_with_analysis(
                input_ids, temperature=0.8, top_k=10
            )
            
            # è§£ç topé¢„æµ‹
            top_predictions = []
            for token_id, prob in analysis["top_tokens"]:
                token = self.tokenizer.id_to_word[token_id]
                top_predictions.append({"token": token, "prob": float(prob)})
            
            results["token_predictions"].append({
                "input_text": text[:30],
                "predictions": top_predictions,
                "entropy": analysis["entropy"],
                "perplexity": analysis["perplexity"]
            })
            
            # ç”Ÿæˆæ–‡æœ¬åˆ†æ
            generated = self._generate_with_analysis(text[:20], max_length=15)
            results["generation_analysis"].append(generated)
            
            # æ¦‚ç‡åˆ†å¸ƒåˆ†æ
            prob_dist = self._analyze_probability_distribution(analysis["full_probs"])
            results["probability_distributions"].append(prob_dist)
            
            # æ³¨æ„åŠ›æ¨¡å¼åˆ†æ
            results["attention_patterns"].append(analysis["attention_analysis"])
        
        # ä¿å­˜ç»“æœå¹¶å¯è§†åŒ–
        self._save_and_visualize_analysis(results, save_dir)
        
        return results
    
    def _generate_with_analysis(self, seed_text: str, max_length: int = 20) -> Dict:
        """å¸¦åˆ†æçš„æ–‡æœ¬ç”Ÿæˆ"""
        current_text = seed_text
        generation_steps = []
        
        for step in range(max_length):
            # ç¼–ç å½“å‰æ–‡æœ¬
            input_tokens = self.tokenizer.encode(current_text, level="word")
            input_ids = torch.tensor([input_tokens[-32:]]).to(self.device)  # ä½¿ç”¨æœ€å32ä¸ªtoken
            
            # é¢„æµ‹ä¸‹ä¸€ä¸ªtoken
            analysis = self.model.predict_next_token_with_analysis(input_ids, temperature=0.7)
            
            # é€‰æ‹©æœ€å¯èƒ½çš„token
            next_token_id, next_prob = analysis["top_tokens"][0]
            next_token = self.tokenizer.id_to_word[next_token_id]
            
            # è®°å½•ç”Ÿæˆæ­¥éª¤
            generation_steps.append({
                "step": step + 1,
                "input_context": current_text[-20:],  # æœ€å20ä¸ªå­—ç¬¦
                "predicted_token": next_token,
                "probability": float(next_prob),
                "entropy": analysis["entropy"],
                "perplexity": analysis["perplexity"]
            })
            
            # æ›´æ–°æ–‡æœ¬
            current_text += next_token
            
            # æ£€æŸ¥åœæ­¢æ¡ä»¶
            if next_token in ['<EOS>', '<PAD>'] or len(current_text) > len(seed_text) + 50:
                break
        
        return {
            "seed_text": seed_text,
            "generated_text": current_text,
            "generation_steps": generation_steps
        }
    
    def _analyze_probability_distribution(self, probs: np.ndarray) -> Dict:
        """åˆ†ææ¦‚ç‡åˆ†å¸ƒ"""
        # è®¡ç®—åˆ†å¸ƒç»Ÿè®¡
        entropy = -np.sum(probs * np.log(probs + 1e-10))
        max_prob = np.max(probs)
        top_10_mass = np.sum(np.sort(probs)[-10:])
        
        # æ‰¾åˆ°é«˜æ¦‚ç‡åŒºé—´
        sorted_indices = np.argsort(probs)[::-1]
        cumulative_prob = 0
        tokens_for_90_percent = 0
        
        for i in sorted_indices:
            cumulative_prob += probs[i]
            tokens_for_90_percent += 1
            if cumulative_prob >= 0.9:
                break
        
        return {
            "entropy": float(entropy),
            "max_probability": float(max_prob),
            "top_10_mass": float(top_10_mass),
            "tokens_for_90_percent": tokens_for_90_percent,
            "distribution_shape": "peaked" if max_prob > 0.5 else "flat"
        }
    
    def _save_and_visualize_analysis(self, results: Dict, save_dir: str):
        """ä¿å­˜å¹¶å¯è§†åŒ–åˆ†æç»“æœ"""
        os.makedirs(save_dir, exist_ok=True)
        
        # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
        serializable_results = self._convert_numpy_to_list(results)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        with open(os.path.join(save_dir, "comprehensive_analysis.json"), 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, ensure_ascii=False, indent=2)
        
        # å¯è§†åŒ–1: è®­ç»ƒè¿‡ç¨‹
        self._plot_training_progress(save_dir)
        
        # å¯è§†åŒ–2: Tokené¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ
        self._plot_prediction_analysis(results, save_dir)
        
        # å¯è§†åŒ–3: ç”Ÿæˆè¿‡ç¨‹åˆ†æ
        self._plot_generation_analysis(results, save_dir)
        
        # å¯è§†åŒ–4: æ³¨æ„åŠ›æ¨¡å¼
        self._plot_attention_patterns(results, save_dir)
        
        print("âœ… åˆ†æç»“æœå·²ä¿å­˜å¹¶å¯è§†åŒ–")
    
    def _convert_numpy_to_list(self, obj):
        """é€’å½’è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨"""
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {key: self._convert_numpy_to_list(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_numpy_to_list(item) for item in obj]
        else:
            return obj
    
    def _plot_training_progress(self, save_dir: str):
        """ç»˜åˆ¶è®­ç»ƒè¿›åº¦"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        epochs = range(1, len(self.training_stats["losses"]) + 1)
        
        # æŸå¤±æ›²çº¿
        ax1.plot(epochs, self.training_stats["losses"], 'b-', linewidth=2)
        ax1.set_title('è®­ç»ƒæŸå¤±')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.grid(True, alpha=0.3)
        
        # å›°æƒ‘åº¦æ›²çº¿
        ax2.plot(epochs, self.training_stats["perplexities"], 'r-', linewidth=2)
        ax2.set_title('å›°æƒ‘åº¦')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Perplexity')
        ax2.grid(True, alpha=0.3)
        
        # å­¦ä¹ ç‡æ›²çº¿
        ax3.plot(epochs, self.training_stats["learning_rates"], 'g-', linewidth=2)
        ax3.set_title('å­¦ä¹ ç‡')
        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('Learning Rate')
        ax3.grid(True, alpha=0.3)
        
        # æŸå¤±å’Œå›°æƒ‘åº¦å¯¹æ¯”
        ax4_twin = ax4.twinx()
        line1 = ax4.plot(epochs, self.training_stats["losses"], 'b-', label='Loss')
        line2 = ax4_twin.plot(epochs, self.training_stats["perplexities"], 'r-', label='Perplexity')
        
        ax4.set_xlabel('Epoch')
        ax4.set_ylabel('Loss', color='b')
        ax4_twin.set_ylabel('Perplexity', color='r')
        ax4.set_title('è®­ç»ƒæŒ‡æ ‡å¯¹æ¯”')
        
        # åˆå¹¶å›¾ä¾‹
        lines = line1 + line2
        labels = [l.get_label() for l in lines]
        ax4.legend(lines, labels, loc='upper right')
        
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'training_progress.png'), dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_prediction_analysis(self, results: Dict, save_dir: str):
        """ç»˜åˆ¶é¢„æµ‹åˆ†æ"""
        predictions = results["token_predictions"]
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        axes = axes.flatten()
        
        for i, pred_data in enumerate(predictions):
            if i >= 4:
                break
            
            # æå–é¢„æµ‹æ•°æ®
            tokens = [p["token"] for p in pred_data["predictions"][:8]]
            probs = [p["prob"] for p in pred_data["predictions"][:8]]
            
            # æ¸…ç†tokenæ˜¾ç¤º
            clean_tokens = [token.replace('\n', '\\n').replace(' ', '_') for token in tokens]
            
            # ç»˜åˆ¶æŸ±çŠ¶å›¾
            bars = axes[i].bar(range(len(clean_tokens)), probs, 
                              color=plt.cm.viridis(np.linspace(0, 1, len(probs))), alpha=0.8)
            
            axes[i].set_xlabel('Tokenæ’å')
            axes[i].set_ylabel('é¢„æµ‹æ¦‚ç‡')
            axes[i].set_title(f'æ–‡æœ¬ {i+1} çš„Tokené¢„æµ‹\n'
                             f'ç†µ: {pred_data["entropy"]:.2f}, '
                             f'å›°æƒ‘åº¦: {pred_data["perplexity"]:.2f}')
            axes[i].set_xticks(range(len(clean_tokens)))
            axes[i].set_xticklabels(clean_tokens, rotation=45, ha='right')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for j, (bar, prob) in enumerate(zip(bars, probs)):
                height = bar.get_height()
                axes[i].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                           f'{prob:.3f}', ha='center', va='bottom', fontsize=8)
        
        # éšè—å¤šä½™çš„subplot
        for i in range(len(predictions), 4):
            axes[i].set_visible(False)
        
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'prediction_analysis.png'), dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_generation_analysis(self, results: Dict, save_dir: str):
        """ç»˜åˆ¶ç”Ÿæˆè¿‡ç¨‹åˆ†æ"""
        generation_data = results["generation_analysis"]
        
        fig, axes = plt.subplots(len(generation_data), 1, figsize=(12, 4*len(generation_data)))
        if len(generation_data) == 1:
            axes = [axes]
        
        for i, gen_data in enumerate(generation_data):
            steps = [step["step"] for step in gen_data["generation_steps"]]
            probs = [step["probability"] for step in gen_data["generation_steps"]]
            entropies = [step["entropy"] for step in gen_data["generation_steps"]]
            
            # åŒyè½´å›¾
            ax1 = axes[i]
            ax2 = ax1.twinx()
            
            # æ¦‚ç‡æ›²çº¿
            line1 = ax1.plot(steps, probs, 'b-o', label='é¢„æµ‹æ¦‚ç‡', linewidth=2, markersize=4)
            ax1.set_xlabel('ç”Ÿæˆæ­¥éª¤')
            ax1.set_ylabel('é¢„æµ‹æ¦‚ç‡', color='b')
            ax1.tick_params(axis='y', labelcolor='b')
            
            # ç†µæ›²çº¿
            line2 = ax2.plot(steps, entropies, 'r-s', label='ä¿¡æ¯ç†µ', linewidth=2, markersize=4)
            ax2.set_ylabel('ä¿¡æ¯ç†µ', color='r')
            ax2.tick_params(axis='y', labelcolor='r')
            
            ax1.set_title(f'ç”Ÿæˆè¿‡ç¨‹ {i+1}: {gen_data["seed_text"]} â†’ {gen_data["generated_text"][:50]}...')
            ax1.grid(True, alpha=0.3)
            
            # åˆå¹¶å›¾ä¾‹
            lines = line1 + line2
            labels = [l.get_label() for l in lines]
            ax1.legend(lines, labels, loc='upper left')
        
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'generation_analysis.png'), dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_attention_patterns(self, results: Dict, save_dir: str):
        """ç»˜åˆ¶æ³¨æ„åŠ›æ¨¡å¼"""
        attention_data = results["attention_patterns"]
        
        fig, axes = plt.subplots(1, len(attention_data), figsize=(5*len(attention_data), 4))
        if len(attention_data) == 1:
            axes = [axes]
        
        for i, attn_data in enumerate(attention_data):
            attention_weights = attn_data["attention_weights"]
            
            # ç»˜åˆ¶æ³¨æ„åŠ›çƒ­åŠ›å›¾
            im = axes[i].imshow(attention_weights, cmap='Blues', aspect='auto')
            axes[i].set_title(f'æ³¨æ„åŠ›æ¨¡å¼ {i+1}')
            axes[i].set_xlabel('è¾“å…¥ä½ç½®')
            axes[i].set_ylabel('è¾“å‡ºä½ç½®')
            
            # æ·»åŠ é¢œè‰²æ¡
            plt.colorbar(im, ax=axes[i])
        
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'attention_patterns.png'), dpi=300, bbox_inches='tight')
        plt.close()
    
    def save_model_and_tokenizer(self, save_dir: str):
        """ä¿å­˜æ¨¡å‹å’Œåˆ†è¯å™¨"""
        os.makedirs(save_dir, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'model_config': {
                'vocab_size': self.model.vocab_size,
                'd_model': self.model.d_model,
                'max_seq_len': self.model.max_seq_len
            },
            'training_stats': self.training_stats
        }, os.path.join(save_dir, 'transformer_language_model.pth'))
        
        # ä¿å­˜åˆ†è¯å™¨
        with open(os.path.join(save_dir, 'advanced_tokenizer.pkl'), 'wb') as f:
            pickle.dump(self.tokenizer, f)
        
        # ä¿å­˜é…ç½®
        config = {
            "model_type": "transformer",
            "vocab_size": len(self.tokenizer.word_to_id),
            "char_vocab_size": len(self.tokenizer.char_to_id),
            "training_epochs": len(self.training_stats["losses"]),
            "final_loss": self.training_stats["losses"][-1] if self.training_stats["losses"] else None,
            "final_perplexity": self.training_stats["perplexities"][-1] if self.training_stats["perplexities"] else None
        }
        
        with open(os.path.join(save_dir, 'model_config.json'), 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æ¨¡å‹å’Œåˆ†è¯å™¨å·²ä¿å­˜åˆ°: {save_dir}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ Next Token Prediction è¿›é˜¶å®éªŒå¼€å§‹!")
    print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    # å®šä¹‰è·¯å¾„
    current_dir = os.path.dirname(os.path.abspath(__file__))
    data_file = os.path.join(current_dir, "text_generation_data.json")
    results_dir = os.path.join(current_dir, "results")
    checkpoint_dir = os.path.join(current_dir, "model-checkpoint")
    
    try:
        # 1. åˆ›å»ºå®éªŒå®ä¾‹
        lab = NextTokenPredictionLab(data_file)
        
        # 2. åŠ è½½å’Œå‡†å¤‡æ•°æ®
        texts = lab.load_and_prepare_data()
        
        # 3. åˆ›å»ºTransformeræ¨¡å‹
        lab.create_model("transformer")
        
        # 4. è®­ç»ƒæ¨¡å‹
        lab.train_model(texts, epochs=25, batch_size=8)
        
        # 5. ç»¼åˆåˆ†æ
        analysis_results = lab.comprehensive_analysis(texts, results_dir)
        
        # 6. ä¿å­˜æ¨¡å‹
        lab.save_model_and_tokenizer(checkpoint_dir)
        
        print("\n" + "=" * 60)
        print("ğŸ‰ Next Token Prediction è¿›é˜¶å®éªŒå®Œæˆ!")
        print(f"ğŸ“ åˆ†æç»“æœ: {results_dir}")
        print(f"ğŸ¤– æ¨¡å‹æ£€æŸ¥ç‚¹: {checkpoint_dir}")
        
        # æ‰“å°å®éªŒæ‘˜è¦
        print("\nğŸ“Š å®éªŒæ‘˜è¦:")
        print(f"âœ… è®­ç»ƒè½®æ•°: {len(lab.training_stats['losses'])}")
        print(f"âœ… æœ€ç»ˆæŸå¤±: {lab.training_stats['losses'][-1]:.4f}")
        print(f"âœ… æœ€ç»ˆå›°æƒ‘åº¦: {lab.training_stats['perplexities'][-1]:.2f}")
        print(f"âœ… åˆ†ææ–‡æœ¬æ•°é‡: {len(analysis_results['token_predictions'])}")
        
    except Exception as e:
        print(f"\nâŒ å®éªŒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
