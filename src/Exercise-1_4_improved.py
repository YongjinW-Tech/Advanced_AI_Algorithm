#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»ƒä¹ 4æ”¹è¿›ç‰ˆï¼šç”¨æˆ·æ•°æ®å‘é‡åŒ–ä¸é™ç»´å¯è§†åŒ–ä¼˜åŒ–

ç›®æ ‡ï¼šä¼˜åŒ–é™ç»´æ•ˆæœï¼Œæé«˜ç±»åˆ«åŒºåˆ†åº¦
æ”¹è¿›ç­–ç•¥ï¼š
1. t-SNEå’ŒUMAPéçº¿æ€§é™ç»´
2. æ ‡å‡†åŒ–å’Œç‰¹å¾å·¥ç¨‹
3. å¤šç§å¯è§†åŒ–å¯¹æ¯”
4. ç‰¹å¾é€‰æ‹©å’Œæƒé‡è°ƒæ•´

ä½œè€…ï¼šYjTech
ç‰ˆæœ¬ï¼š2.0
æ—¥æœŸï¼š2025å¹´6æœˆ29æ—¥
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.feature_selection import SelectKBest, f_classif
from sentence_transformers import SentenceTransformer
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = [
    'PingFang SC', 'Hiragino Sans GB', 'STHeiti', 'Microsoft YaHei', 
    'Arial Unicode MS', 'Heiti TC', 'SimHei', 'DejaVu Sans'
]
plt.rcParams['axes.unicode_minus'] = False

class ImprovedUserVectorizer:
    """æ”¹è¿›çš„ç”¨æˆ·æ•°æ®å‘é‡åŒ–å™¨"""
    
    def __init__(self, model_name='BAAI/bge-m3'):
        """
        åˆå§‹åŒ–å‘é‡åŒ–å™¨
        Args:
            model_name: BGE-m3æ¨¡å‹åç§°
        """
        self.model_name = model_name
        self.embedding_model = None
        self.scaler = StandardScaler()
        self.robust_scaler = RobustScaler()
        self.feature_selector = None
        
    def load_embedding_model(self):
        """åŠ è½½BGE-m3åµŒå…¥æ¨¡å‹"""
        print("æ­£åœ¨åŠ è½½BGE-m3åµŒå…¥æ¨¡å‹...")
        try:
            self.embedding_model = SentenceTransformer(self.model_name)
            print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {self.model_name}")
        except Exception as e:
            print(f"âš ï¸  BGE-m3æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ä½¿ç”¨å¤‡é€‰æ¨¡å‹: all-MiniLM-L6-v2")
            self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
    
    def create_enhanced_text_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        åˆ›å»ºå¢å¼ºçš„æ–‡æœ¬ç‰¹å¾
        Args:
            df: åŸå§‹ç”¨æˆ·æ•°æ®
        Returns:
            åŒ…å«å¢å¼ºæ–‡æœ¬ç‰¹å¾çš„DataFrame
        """
        print("åˆ›å»ºå¢å¼ºæ–‡æœ¬ç‰¹å¾...")
        
        result_df = df.copy()
        
        # 1. åŸºç¡€ç±»åˆ«ç‰¹å¾æ–‡æœ¬
        result_df['æ€§åˆ«_æ–‡æœ¬'] = df['æ€§åˆ«'].apply(lambda x: f"{x}æ€§ç”¨æˆ·")
        result_df['åŸå¸‚_æ–‡æœ¬'] = df['æ‰€åœ¨åŸå¸‚'].apply(lambda x: f"æ¥è‡ª{x}")
        result_df['æ¶ˆè´¹_æ–‡æœ¬'] = df['æ¶ˆè´¹æ°´å¹³'].apply(lambda x: f"{x}æ¶ˆè´¹ç¾¤ä½“")
        
        # 2. ç»„åˆç‰¹å¾æ–‡æœ¬
        result_df['æ€§åˆ«åŸå¸‚_æ–‡æœ¬'] = df.apply(lambda x: f"{x['æ€§åˆ«']}æ€§{x['æ‰€åœ¨åŸå¸‚']}ç”¨æˆ·", axis=1)
        result_df['æ¶ˆè´¹å¹´é¾„_æ–‡æœ¬'] = df.apply(lambda x: f"{x['æ¶ˆè´¹æ°´å¹³']}æ¶ˆè´¹{x['å¹´é¾„']}å²ç”¨æˆ·", axis=1)
        
        # 3. å¹´é¾„åˆ†ç»„æ–‡æœ¬
        def age_group(age):
            if age < 25:
                return "å¹´è½»ç”¨æˆ·"
            elif age < 35:
                return "é’å¹´ç”¨æˆ·"
            elif age < 45:
                return "ä¸­å¹´ç”¨æˆ·"
            else:
                return "èµ„æ·±ç”¨æˆ·"
        
        result_df['å¹´é¾„ç»„_æ–‡æœ¬'] = df['å¹´é¾„'].apply(age_group)
        
        # 4. æ´»è·ƒåº¦åˆ†ç»„æ–‡æœ¬
        def activity_group(days):
            if days <= 7:
                return "é«˜æ´»è·ƒç”¨æˆ·"
            elif days <= 30:
                return "ä¸­æ´»è·ƒç”¨æˆ·"
            else:
                return "ä½æ´»è·ƒç”¨æˆ·"
        
        result_df['æ´»è·ƒåº¦_æ–‡æœ¬'] = df['æœ€è¿‘æ´»è·ƒå¤©æ•°'].apply(activity_group)
        
        # 5. ç»¼åˆç”¨æˆ·ç”»åƒ
        result_df['ç”¨æˆ·ç”»åƒ'] = df.apply(
            lambda x: f"{x['æ€§åˆ«']}æ€§{x['å¹´é¾„']}å²{x['æ¶ˆè´¹æ°´å¹³']}æ¶ˆè´¹{x['æ‰€åœ¨åŸå¸‚']}ç”¨æˆ·ï¼Œ"
                     f"æœ€è¿‘{x['æœ€è¿‘æ´»è·ƒå¤©æ•°']}å¤©æ´»è·ƒ", axis=1
        )
        
        print(f"åˆ›å»ºäº† {len([col for col in result_df.columns if '_æ–‡æœ¬' in col or 'ç”»åƒ' in col])} ä¸ªæ–‡æœ¬ç‰¹å¾")
        
        return result_df
    
    def compute_embeddings(self, df: pd.DataFrame) -> dict:
        """è®¡ç®—æ–‡æœ¬åµŒå…¥å‘é‡"""
        print("è®¡ç®—æ–‡æœ¬åµŒå…¥å‘é‡...")
        
        if self.embedding_model is None:
            self.load_embedding_model()
        
        embeddings = {}
        text_columns = [col for col in df.columns if '_æ–‡æœ¬' in col or 'ç”»åƒ' in col]
        
        for col in text_columns:
            print(f"  å¤„ç† {col}...")
            texts = df[col].tolist()
            vectors = self.embedding_model.encode(texts, show_progress_bar=True)
            embeddings[col] = vectors
            print(f"  {col}: {vectors.shape[0]} ä¸ªæ–‡æœ¬ -> {vectors.shape[1]} ç»´å‘é‡")
        
        return embeddings
    
    def create_enhanced_feature_vectors(self, df: pd.DataFrame, embeddings: dict) -> np.ndarray:
        """åˆ›å»ºå¢å¼ºçš„ç‰¹å¾å‘é‡"""
        print("æ‹¼æ¥å¢å¼ºç‰¹å¾å‘é‡...")
        
        feature_components = []
        
        # 1. æ•°å€¼ç‰¹å¾ï¼ˆä½¿ç”¨RobustScalerå‡å°‘å¼‚å¸¸å€¼å½±å“ï¼‰
        numerical_features = ['å¹´é¾„', 'æœ€è¿‘æ´»è·ƒå¤©æ•°']
        numerical_data = df[numerical_features].values
        numerical_data_scaled = self.robust_scaler.fit_transform(numerical_data)
        
        # å¢åŠ æ•°å€¼ç‰¹å¾çš„æƒé‡
        numerical_data_weighted = numerical_data_scaled * 3  # å¢åŠ æƒé‡
        feature_components.append(numerical_data_weighted)
        print(f"  æ•°å€¼ç‰¹å¾: {numerical_data_weighted.shape[1]} ç»´ (åŠ æƒ)")
        
        # 2. åµŒå…¥å‘é‡ç‰¹å¾ï¼ˆæ ‡å‡†åŒ–ï¼‰
        for col, vectors in embeddings.items():
            # å¯¹æ¯ä¸ªåµŒå…¥å‘é‡è¿›è¡ŒL2æ ‡å‡†åŒ–
            vectors_normalized = vectors / (np.linalg.norm(vectors, axis=1, keepdims=True) + 1e-8)
            feature_components.append(vectors_normalized)
            print(f"  {col}: {vectors_normalized.shape[1]} ç»´ (L2æ ‡å‡†åŒ–)")
        
        # 3. æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
        final_vectors = np.hstack(feature_components)
        print(f"æœ€ç»ˆç‰¹å¾å‘é‡: {final_vectors.shape[0]} ä¸ªç”¨æˆ· Ã— {final_vectors.shape[1]} ç»´ç‰¹å¾")
        
        return final_vectors
    
    def apply_feature_selection(self, vectors: np.ndarray, df: pd.DataFrame, 
                              target_feature: str = 'æ¶ˆè´¹æ°´å¹³', k: int = 1000) -> np.ndarray:
        """åº”ç”¨ç‰¹å¾é€‰æ‹©"""
        print(f"åº”ç”¨ç‰¹å¾é€‰æ‹©: {vectors.shape[1]} ç»´ -> {k} ç»´")
        
        # ä½¿ç”¨æ¶ˆè´¹æ°´å¹³ä½œä¸ºç›®æ ‡è¿›è¡Œç‰¹å¾é€‰æ‹©
        from sklearn.preprocessing import LabelEncoder
        le = LabelEncoder()
        y = le.fit_transform(df[target_feature])
        
        self.feature_selector = SelectKBest(score_func=f_classif, k=min(k, vectors.shape[1]))
        vectors_selected = self.feature_selector.fit_transform(vectors, y)
        
        print(f"ç‰¹å¾é€‰æ‹©å®Œæˆ: ä¿ç•™äº† {vectors_selected.shape[1]} ä¸ªæœ€é‡è¦çš„ç‰¹å¾")
        return vectors_selected
    
    def apply_multiple_dimensionality_reduction(self, vectors: np.ndarray) -> dict:
        """åº”ç”¨å¤šç§é™ç»´æ–¹æ³•"""
        print("åº”ç”¨å¤šç§é™ç»´æ–¹æ³•...")
        
        results = {}
        
        # 1. PCAé™ç»´
        print("  æ‰§è¡ŒPCAé™ç»´...")
        pca = PCA(n_components=2, random_state=42)
        vectors_pca = pca.fit_transform(vectors)
        results['PCA'] = {
            'vectors': vectors_pca,
            'model': pca,
            'explained_variance': pca.explained_variance_ratio_.sum()
        }
        print(f"    PCAè§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_.sum():.1%}")
        
        # 2. t-SNEé™ç»´
        print("  æ‰§è¡Œt-SNEé™ç»´...")
        tsne = TSNE(n_components=2, random_state=42, perplexity=30, 
                   learning_rate=200, max_iter=1000, verbose=0)
        vectors_tsne = tsne.fit_transform(vectors)
        results['t-SNE'] = {
            'vectors': vectors_tsne,
            'model': tsne
        }
        print("    t-SNEé™ç»´å®Œæˆ")
        
        # 3. å°è¯•å®‰è£…å’Œä½¿ç”¨UMAP
        try:
            import umap
            print("  æ‰§è¡ŒUMAPé™ç»´...")
            umap_model = umap.UMAP(n_components=2, random_state=42, 
                                  n_neighbors=15, min_dist=0.1)
            vectors_umap = umap_model.fit_transform(vectors)
            results['UMAP'] = {
                'vectors': vectors_umap,
                'model': umap_model
            }
            print("    UMAPé™ç»´å®Œæˆ")
        except ImportError:
            print("    UMAPæœªå®‰è£…ï¼Œè·³è¿‡UMAPé™ç»´")
        
        return results
    
    def visualize_comparison(self, reduction_results: dict, df: pd.DataFrame, save_plot: bool = True):
        """å¯¹æ¯”å¯è§†åŒ–ä¸åŒé™ç»´æ–¹æ³•"""
        print("åˆ›å»ºé™ç»´æ–¹æ³•å¯¹æ¯”å¯è§†åŒ–...")
        
        n_methods = len(reduction_results)
        fig, axes = plt.subplots(2, n_methods, figsize=(6*n_methods, 10))
        if n_methods == 1:
            axes = axes.reshape(2, 1)
        
        fig.suptitle('ä¸åŒé™ç»´æ–¹æ³•æ•ˆæœå¯¹æ¯”', fontsize=16, fontweight='bold')
        
        # é¢œè‰²è®¾ç½®
        gender_colors = {'ç”·': 'blue', 'å¥³': 'red', 'æœªé€éœ²': 'gray'}
        consumption_colors = {'é«˜': 'red', 'ä¸­': 'orange', 'ä½': 'green'}
        
        for i, (method_name, result) in enumerate(reduction_results.items()):
            vectors_2d = result['vectors']
            
            # ç¬¬ä¸€è¡Œï¼šæŒ‰æ€§åˆ«ç€è‰²
            ax1 = axes[0, i]
            for gender, color in gender_colors.items():
                mask = df['æ€§åˆ«'] == gender
                if mask.any():
                    ax1.scatter(vectors_2d[mask, 0], vectors_2d[mask, 1], 
                               c=color, label=gender, alpha=0.6, s=30)
            ax1.set_title(f'{method_name} - æŒ‰æ€§åˆ«åˆ†å¸ƒ')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # ç¬¬äºŒè¡Œï¼šæŒ‰æ¶ˆè´¹æ°´å¹³ç€è‰²
            ax2 = axes[1, i]
            for level, color in consumption_colors.items():
                mask = df['æ¶ˆè´¹æ°´å¹³'] == level
                if mask.any():
                    ax2.scatter(vectors_2d[mask, 0], vectors_2d[mask, 1], 
                               c=color, label=level, alpha=0.6, s=30)
            ax2.set_title(f'{method_name} - æŒ‰æ¶ˆè´¹æ°´å¹³åˆ†å¸ƒ')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
            
            # æ·»åŠ æ–¹å·®è§£é‡Šä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if 'explained_variance' in result:
                ax1.text(0.02, 0.98, f'è§£é‡Šæ–¹å·®: {result["explained_variance"]:.1%}', 
                        transform=ax1.transAxes, verticalalignment='top',
                        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        
        plt.tight_layout()
        
        if save_plot:
            plt.savefig('./output/dimensionality_reduction_comparison.png', dpi=300, bbox_inches='tight')
            print("å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜ä¸º './output/dimensionality_reduction_comparison.png'")
        
        plt.show()
    
    def calculate_separation_metrics(self, vectors_2d: np.ndarray, df: pd.DataFrame, 
                                   feature: str = 'æ¶ˆè´¹æ°´å¹³') -> dict:
        """è®¡ç®—ç±»åˆ«åˆ†ç¦»åº¦æŒ‡æ ‡"""
        print(f"è®¡ç®— {feature} çš„ç±»åˆ«åˆ†ç¦»åº¦...")
        
        categories = df[feature].unique()
        metrics = {}
        
        # è®¡ç®—ç±»å†…è·ç¦»å’Œç±»é—´è·ç¦»
        intra_distances = []
        inter_distances = []
        
        for cat in categories:
            mask = df[feature] == cat
            if mask.sum() > 1:
                cat_points = vectors_2d[mask]
                cat_center = cat_points.mean(axis=0)
                
                # ç±»å†…è·ç¦»ï¼ˆåˆ°ä¸­å¿ƒçš„å¹³å‡è·ç¦»ï¼‰
                intra_dist = np.mean(np.linalg.norm(cat_points - cat_center, axis=1))
                intra_distances.append(intra_dist)
                
                # ä¸å…¶ä»–ç±»åˆ«ä¸­å¿ƒçš„è·ç¦»
                for other_cat in categories:
                    if other_cat != cat:
                        other_mask = df[feature] == other_cat
                        if other_mask.sum() > 0:
                            other_center = vectors_2d[other_mask].mean(axis=0)
                            inter_dist = np.linalg.norm(cat_center - other_center)
                            inter_distances.append(inter_dist)
        
        avg_intra = np.mean(intra_distances) if intra_distances else 0
        avg_inter = np.mean(inter_distances) if inter_distances else 0
        
        # åˆ†ç¦»åº¦æŒ‡æ ‡ï¼šç±»é—´è·ç¦»/ç±»å†…è·ç¦»
        separation_ratio = avg_inter / avg_intra if avg_intra > 0 else 0
        
        metrics = {
            'avg_intra_distance': avg_intra,
            'avg_inter_distance': avg_inter,
            'separation_ratio': separation_ratio
        }
        
        print(f"  å¹³å‡ç±»å†…è·ç¦»: {avg_intra:.3f}")
        print(f"  å¹³å‡ç±»é—´è·ç¦»: {avg_inter:.3f}")
        print(f"  åˆ†ç¦»åº¦æŒ‡æ ‡: {separation_ratio:.3f}")
        
        return metrics

def install_advanced_requirements():
    """å®‰è£…é«˜çº§ä¾èµ–åŒ…"""
    print("æ£€æŸ¥å¹¶å®‰è£…é«˜çº§ä¾èµ–åŒ…...")
    
    packages_to_check = [
        ('sentence_transformers', 'sentence-transformers'),
        ('umap', 'umap-learn')
    ]
    
    for package_name, pip_name in packages_to_check:
        try:
            __import__(package_name)
            print(f"âœ… {pip_name} å·²å®‰è£…")
        except ImportError:
            print(f"âš ï¸  æ­£åœ¨å®‰è£… {pip_name}...")
            import subprocess
            subprocess.check_call(["pip", "install", pip_name])
            print(f"âœ… {pip_name} å®‰è£…å®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    print("ç»ƒä¹ 4æ”¹è¿›ç‰ˆï¼šç”¨æˆ·æ•°æ®å‘é‡åŒ–ä¸é™ç»´å¯è§†åŒ–ä¼˜åŒ–")
    print("="*60)
    
    # æ£€æŸ¥ä¾èµ–
    install_advanced_requirements()
    
    # åŠ è½½æ•°æ®
    try:
        df = pd.read_csv('./output/user_profiles.csv')
        print(f"æˆåŠŸåŠ è½½ç”¨æˆ·æ•°æ®: {len(df)} æ¡è®°å½•")
    except FileNotFoundError:
        print("æœªæ‰¾åˆ°ç”¨æˆ·æ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ Exercise-1_1.py ç”Ÿæˆæ•°æ®")
        return
    
    # åˆ›å»ºæ”¹è¿›çš„å‘é‡åŒ–å™¨
    vectorizer = ImprovedUserVectorizer()
    
    # æ­¥éª¤1: åˆ›å»ºå¢å¼ºæ–‡æœ¬ç‰¹å¾
    df_with_text = vectorizer.create_enhanced_text_features(df)
    
    # æ­¥éª¤2: è®¡ç®—åµŒå…¥å‘é‡
    embeddings = vectorizer.compute_embeddings(df_with_text)
    
    # æ­¥éª¤3: åˆ›å»ºå¢å¼ºç‰¹å¾å‘é‡
    feature_vectors = vectorizer.create_enhanced_feature_vectors(df, embeddings)
    
    # æ­¥éª¤4: ç‰¹å¾é€‰æ‹©ï¼ˆå¯é€‰ï¼‰
    if feature_vectors.shape[1] > 1000:
        feature_vectors_selected = vectorizer.apply_feature_selection(
            feature_vectors, df, target_feature='æ¶ˆè´¹æ°´å¹³', k=1000
        )
    else:
        feature_vectors_selected = feature_vectors
    
    # æ­¥éª¤5: åº”ç”¨å¤šç§é™ç»´æ–¹æ³•
    reduction_results = vectorizer.apply_multiple_dimensionality_reduction(feature_vectors_selected)
    
    # æ­¥éª¤6: å¯¹æ¯”å¯è§†åŒ–
    vectorizer.visualize_comparison(reduction_results, df, save_plot=True)
    
    # æ­¥éª¤7: è®¡ç®—åˆ†ç¦»åº¦æŒ‡æ ‡
    print(f"\n{'='*50}")
    print("é™ç»´æ–¹æ³•æ•ˆæœè¯„ä¼°")
    print(f"{'='*50}")
    
    for method_name, result in reduction_results.items():
        print(f"\n{method_name} æ–¹æ³•:")
        metrics = vectorizer.calculate_separation_metrics(
            result['vectors'], df, feature='æ¶ˆè´¹æ°´å¹³'
        )
        
        # ä¿å­˜ç»“æœ
        result_df = df.copy()
        result_df[f'{method_name}_X'] = result['vectors'][:, 0]
        result_df[f'{method_name}_Y'] = result['vectors'][:, 1]
        
        output_file = f'./output/user_vectors_{method_name.lower()}.csv'
        result_df.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"  ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # æ¨èæœ€ä½³æ–¹æ³•
    print(f"\n{'='*50}")
    print("æ”¹è¿›å»ºè®®")
    print(f"{'='*50}")
    
    print("""
    åŸºäºåˆ†æç»“æœï¼Œä»¥ä¸‹æ˜¯æ”¹è¿›é™ç»´æ•ˆæœçš„å»ºè®®ï¼š
    
    1. ğŸ“Š æ•°æ®å±‚é¢æ”¹è¿›ï¼š
       - å¢åŠ æ›´å¤šæœ‰åŒºåˆ†æ€§çš„ç‰¹å¾
       - å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
       - æ”¶é›†æ›´å¤šæ ·æœ¬æ•°æ®
    
    2. ğŸ”§ æŠ€æœ¯å±‚é¢æ”¹è¿›ï¼š
       - å°è¯•ä¸åŒçš„æ–‡æœ¬æ„é€ ç­–ç•¥
       - è°ƒæ•´ç‰¹å¾æƒé‡å’Œç»„åˆæ–¹å¼
       - ä½¿ç”¨é›†æˆé™ç»´æ–¹æ³•
    
    3. ğŸ“ˆ å¯è§†åŒ–æ”¹è¿›ï¼š
       - å°è¯•3Då¯è§†åŒ–
       - ä½¿ç”¨å¯†åº¦å›¾æ˜¾ç¤ºèšç±»
       - æ·»åŠ å†³ç­–è¾¹ç•Œå¯è§†åŒ–
    
    4. ğŸ¯ è¯„ä¼°æ”¹è¿›ï¼š
       - ä½¿ç”¨èšç±»è´¨é‡æŒ‡æ ‡
       - è®¡ç®—åˆ†ç±»å‡†ç¡®ç‡
       - è¿›è¡Œäº¤å‰éªŒè¯
    """)
    
    return reduction_results

if __name__ == "__main__":
    results = main()
