#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Next Token Prediction ç®€åŒ–æ¼”ç¤ºç‰ˆæœ¬
ä½¿ç”¨æœ¬åœ°è®­ç»ƒçš„ç®€å•æ¨¡å‹æ¼”ç¤º Next Token Prediction åŸç†

Author: AI Algorithm Course
Date: 2025-09-11
"""

import json
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
import numpy as np
from typing import List, Dict, Tuple
import re
from collections import Counter, defaultdict
import pickle

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class SimpleTokenizer:
    """ç®€å•çš„ä¸­æ–‡åˆ†è¯å™¨"""
    
    def __init__(self):
        self.char_to_id = {}
        self.id_to_char = {}
        self.vocab_size = 0
        
    def build_vocab(self, texts: List[str]):
        """æ„å»ºè¯æ±‡è¡¨"""
        # æ”¶é›†æ‰€æœ‰å­—ç¬¦
        all_chars = set()
        for text in texts:
            # ç®€å•çš„å­—ç¬¦çº§åˆ†è¯
            chars = list(text)
            all_chars.update(chars)
        
        # æ·»åŠ ç‰¹æ®Štoken
        special_tokens = ['<PAD>', '<UNK>', '<BOS>', '<EOS>']
        
        # æ„å»ºæ˜ å°„
        vocab = special_tokens + sorted(list(all_chars))
        self.char_to_id = {char: i for i, char in enumerate(vocab)}
        self.id_to_char = {i: char for i, char in enumerate(vocab)}
        self.vocab_size = len(vocab)
        
        print(f"è¯æ±‡è¡¨å¤§å°: {self.vocab_size}")
        
    def encode(self, text: str, max_length: int = None) -> List[int]:
        """ç¼–ç æ–‡æœ¬"""
        chars = list(text)
        ids = [self.char_to_id.get(char, self.char_to_id['<UNK>']) for char in chars]
        
        if max_length:
            if len(ids) > max_length:
                ids = ids[:max_length]
            else:
                ids.extend([self.char_to_id['<PAD>']] * (max_length - len(ids)))
        
        return ids
    
    def decode(self, ids: List[int]) -> str:
        """è§£ç IDåºåˆ—"""
        chars = [self.id_to_char.get(id, '<UNK>') for id in ids]
        # ç§»é™¤ç‰¹æ®Štoken
        chars = [char for char in chars if char not in ['<PAD>', '<UNK>', '<BOS>', '<EOS>']]
        return ''.join(chars)

class SimpleLanguageModel(nn.Module):
    """ç®€å•çš„è¯­è¨€æ¨¡å‹ç”¨äºæ¼”ç¤ºNext Token Prediction"""
    
    def __init__(self, vocab_size: int, embed_dim: int = 128, hidden_dim: int = 256, num_layers: int = 2):
        super().__init__()
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        
        # è¯åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        
        # LSTMå±‚
        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=0.1)
        
        # è¾“å‡ºæŠ•å½±å±‚
        self.output_proj = nn.Linear(hidden_dim, vocab_size)
        
        # Dropout
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, input_ids, hidden=None):
        """å‰å‘ä¼ æ’­"""
        # è¯åµŒå…¥
        embeddings = self.embedding(input_ids)  # [batch, seq_len, embed_dim]
        embeddings = self.dropout(embeddings)
        
        # LSTM
        lstm_out, hidden = self.lstm(embeddings, hidden)  # [batch, seq_len, hidden_dim]
        lstm_out = self.dropout(lstm_out)
        
        # è¾“å‡ºæŠ•å½±
        logits = self.output_proj(lstm_out)  # [batch, seq_len, vocab_size]
        
        return logits, hidden
    
    def predict_next_token(self, input_ids, temperature=1.0, top_k=10):
        """é¢„æµ‹ä¸‹ä¸€ä¸ªtoken"""
        self.eval()
        with torch.no_grad():
            logits, _ = self.forward(input_ids)
            # å–æœ€åä¸€ä¸ªä½ç½®çš„logits
            last_logits = logits[0, -1, :] / temperature
            
            # åº”ç”¨softmax
            probs = F.softmax(last_logits, dim=-1)
            
            # è·å–top-k
            top_probs, top_indices = torch.topk(probs, top_k)
            
            return top_indices.cpu().numpy(), top_probs.cpu().numpy()

class TextDataset(Dataset):
    """æ–‡æœ¬æ•°æ®é›†"""
    
    def __init__(self, texts: List[str], tokenizer: SimpleTokenizer, seq_len: int = 50):
        self.tokenizer = tokenizer
        self.seq_len = seq_len
        self.data = []
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        for text in texts:
            # ç¼–ç æ–‡æœ¬
            encoded = tokenizer.encode(text)
            
            # åˆ›å»ºè¾“å…¥-è¾“å‡ºå¯¹
            for i in range(len(encoded) - seq_len):
                input_seq = encoded[i:i+seq_len]
                target_seq = encoded[i+1:i+seq_len+1]
                self.data.append((input_seq, target_seq))
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        input_seq, target_seq = self.data[idx]
        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)

class NextTokenPredictionDemo:
    """Next Token Prediction æ¼”ç¤ºç±»"""
    
    def __init__(self, data_file: str):
        self.data_file = data_file
        self.tokenizer = SimpleTokenizer()
        self.model = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.training_history = {"losses": []}
        
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def load_data(self):
        """åŠ è½½è®­ç»ƒæ•°æ®"""
        with open(self.data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬æ•°æ®
        texts = []
        texts.extend(data.get("prompts", []))
        texts.extend(data.get("contexts", []))
        
        print(f"åŠ è½½äº† {len(texts)} ä¸ªæ–‡æœ¬æ ·æœ¬")
        return texts
    
    def prepare_model(self, texts: List[str]):
        """å‡†å¤‡æ¨¡å‹å’Œåˆ†è¯å™¨"""
        # æ„å»ºè¯æ±‡è¡¨
        self.tokenizer.build_vocab(texts)
        
        # åˆ›å»ºæ¨¡å‹
        self.model = SimpleLanguageModel(
            vocab_size=self.tokenizer.vocab_size,
            embed_dim=128,
            hidden_dim=256,
            num_layers=2
        ).to(self.device)
        
        print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
    
    def train_model(self, texts: List[str], epochs: int = 10, batch_size: int = 32):
        """è®­ç»ƒæ¨¡å‹"""
        print("\nå¼€å§‹è®­ç»ƒæ¨¡å‹...")
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = TextDataset(texts, self.tokenizer, seq_len=30)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)
        criterion = nn.CrossEntropyLoss(ignore_index=self.tokenizer.char_to_id['<PAD>'])
        
        self.model.train()
        for epoch in range(epochs):
            total_loss = 0
            num_batches = 0
            
            for batch_inputs, batch_targets in dataloader:
                batch_inputs = batch_inputs.to(self.device)
                batch_targets = batch_targets.to(self.device)
                
                optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                logits, _ = self.model(batch_inputs)
                
                # è®¡ç®—æŸå¤±
                loss = criterion(logits.reshape(-1, self.tokenizer.vocab_size), 
                               batch_targets.reshape(-1))
                
                # åå‘ä¼ æ’­
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                num_batches += 1
            
            avg_loss = total_loss / num_batches
            self.training_history["losses"].append(avg_loss)
            
            if (epoch + 1) % 2 == 0:
                print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")
        
        print("è®­ç»ƒå®Œæˆ!")
    
    def demonstrate_next_token_prediction(self, test_texts: List[str], save_dir: str):
        """æ¼”ç¤ºNext Tokené¢„æµ‹"""
        print("\næ¼”ç¤ºNext Tokené¢„æµ‹...")
        
        results = {
            "predictions": [],
            "generation_examples": []
        }
        
        for i, text in enumerate(test_texts[:3]):
            # ä½¿ç”¨æ–‡æœ¬çš„å‰ä¸€éƒ¨åˆ†ä½œä¸ºè¾“å…¥
            prefix = text[:len(text)//2]
            print(f"\næµ‹è¯•æ–‡æœ¬ {i+1}: {prefix}")
            
            # ç¼–ç è¾“å…¥
            input_ids = torch.tensor([self.tokenizer.encode(prefix[-20:])]).to(self.device)
            
            # é¢„æµ‹ä¸‹ä¸€ä¸ªtoken
            top_indices, top_probs = self.model.predict_next_token(input_ids, top_k=5)
            
            predictions = []
            print("Top-5 é¢„æµ‹:")
            for j, (idx, prob) in enumerate(zip(top_indices, top_probs)):
                char = self.tokenizer.id_to_char[idx]
                print(f"  {j+1}. '{char}' (æ¦‚ç‡: {prob:.4f})")
                predictions.append({"char": char, "prob": float(prob)})
            
            results["predictions"].append({
                "prefix": prefix,
                "predictions": predictions
            })
            
            # ç”Ÿæˆæ–‡æœ¬ç¤ºä¾‹
            generated = self.generate_text(prefix[-10:], max_length=20)
            print(f"ç”Ÿæˆæ–‡æœ¬: {generated}")
            
            results["generation_examples"].append({
                "prefix": prefix[-10:],
                "generated": generated
            })
        
        # ä¿å­˜ç»“æœ
        os.makedirs(save_dir, exist_ok=True)
        with open(os.path.join(save_dir, "next_token_demo_results.json"), 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        return results
    
    def generate_text(self, seed_text: str, max_length: int = 30, temperature: float = 0.8):
        """ç”Ÿæˆæ–‡æœ¬"""
        self.model.eval()
        
        # ç¼–ç ç§å­æ–‡æœ¬
        current_text = seed_text
        input_ids = self.tokenizer.encode(current_text)
        
        with torch.no_grad():
            for _ in range(max_length):
                # å‡†å¤‡è¾“å…¥
                input_tensor = torch.tensor([input_ids[-20:]]).to(self.device)  # ä½¿ç”¨æœ€å20ä¸ªå­—ç¬¦
                
                # é¢„æµ‹ä¸‹ä¸€ä¸ªtoken
                top_indices, top_probs = self.model.predict_next_token(input_tensor, temperature=temperature, top_k=3)
                
                # éšæœºé‡‡æ ·
                probs = torch.tensor(top_probs)
                next_idx = torch.multinomial(probs, 1).item()
                next_token_id = top_indices[next_idx]
                
                # æ·»åŠ åˆ°åºåˆ—
                input_ids.append(next_token_id)
                
                # è§£ç æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢
                next_char = self.tokenizer.id_to_char[next_token_id]
                if next_char in ['<EOS>', '<PAD>']:
                    break
                
                current_text += next_char
        
        return current_text
    
    def visualize_training_process(self, save_dir: str):
        """å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹"""
        if not self.training_history["losses"]:
            return
        
        plt.figure(figsize=(10, 6))
        plt.plot(self.training_history["losses"], linewidth=2)
        plt.title('è®­ç»ƒæŸå¤±å˜åŒ–')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'training_loss.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        print("âœ… è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–å·²ä¿å­˜")
    
    def save_model(self, save_dir: str):
        """ä¿å­˜æ¨¡å‹"""
        os.makedirs(save_dir, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹çŠ¶æ€
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'vocab_size': self.tokenizer.vocab_size,
            'char_to_id': self.tokenizer.char_to_id,
            'id_to_char': self.tokenizer.id_to_char,
            'training_history': self.training_history
        }, os.path.join(save_dir, 'simple_language_model.pth'))
        
        # ä¿å­˜åˆ†è¯å™¨
        with open(os.path.join(save_dir, 'tokenizer.pkl'), 'wb') as f:
            pickle.dump(self.tokenizer, f)
        
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {save_dir}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ Next Token Prediction æ¼”ç¤ºå¼€å§‹!")
    print("=" * 50)
    
    # å®šä¹‰è·¯å¾„
    current_dir = os.path.dirname(os.path.abspath(__file__))
    data_file = os.path.join(current_dir, "text_generation_data.json")
    results_dir = os.path.join(current_dir, "results")
    checkpoint_dir = os.path.join(current_dir, "model-checkpoint")
    
    try:
        # åˆ›å»ºæ¼”ç¤ºå®ä¾‹
        demo = NextTokenPredictionDemo(data_file)
        
        # åŠ è½½æ•°æ®
        texts = demo.load_data()
        
        # å‡†å¤‡æ¨¡å‹
        demo.prepare_model(texts)
        
        # è®­ç»ƒæ¨¡å‹
        demo.train_model(texts, epochs=15, batch_size=16)
        
        # æ¼”ç¤ºNext Tokené¢„æµ‹
        results = demo.demonstrate_next_token_prediction(texts, results_dir)
        
        # å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
        demo.visualize_training_process(results_dir)
        
        # ä¿å­˜æ¨¡å‹
        demo.save_model(checkpoint_dir)
        
        print("\n" + "=" * 50)
        print("ğŸ‰ Next Token Prediction æ¼”ç¤ºå®Œæˆ!")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {results_dir}")
        print(f"ğŸ¤– æ¨¡å‹ä¿å­˜åœ¨: {checkpoint_dir}")
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
