#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Next Token Prediction å®éªŒ
åŸºäº OPT æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼ŒæŒæ¡ Next Token Prediction åŸç†

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. åŠ è½½é¢„è®­ç»ƒçš„ OPT æ¨¡å‹
2. å®ç° Next Token Prediction æœºåˆ¶
3. è¿›è¡Œæ–‡æœ¬ç”Ÿæˆå®éªŒ
4. åˆ†æç”Ÿæˆè´¨é‡å’Œæœºåˆ¶

Author: AI Algorithm Course
Date: 2025-09-11
"""

import json
import os
import torch
import torch.nn.functional as F
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    pipeline,
    set_seed
)
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from tqdm import tqdm
import numpy as np
from typing import List, Dict, Tuple
import warnings
warnings.filterwarnings("ignore")

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class NextTokenPredictor:
    """Next Token Prediction å®éªŒç±»"""
    
    def __init__(self, model_name: str = "facebook/opt-350m", device: str = None):
        """
        åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨
        
        Args:
            model_name: é¢„è®­ç»ƒæ¨¡å‹åç§°
            device: è®¡ç®—è®¾å¤‡
        """
        self.device = device if device else ("cuda" if torch.cuda.is_available() else "cpu")
        self.model_name = model_name
        
        print(f"ğŸš€ åˆå§‹åŒ– Next Token Predictor...")
        print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"ğŸ¤– æ¨¡å‹: {model_name}")
        
        # åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
        self.tokenizer = self._load_tokenizer()
        self.model = self._load_model()
        
        # åˆ›å»ºç”Ÿæˆpipeline
        self.generator = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            device=0 if self.device == "cuda" else -1
        )
        
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ!")
    
    def _load_tokenizer(self) -> AutoTokenizer:
        """åŠ è½½åˆ†è¯å™¨"""
        try:
            tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            # è®¾ç½®pad_tokenï¼ˆå¦‚æœæ²¡æœ‰çš„è¯ï¼‰
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            return tokenizer
        except Exception as e:
            print(f"âŒ åˆ†è¯å™¨åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _load_model(self) -> AutoModelForCausalLM:
        """åŠ è½½æ¨¡å‹"""
        try:
            model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None
            )
            model.eval()
            return model
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def predict_next_token(self, text: str, top_k: int = 10) -> List[Tuple[str, float]]:
        """
        é¢„æµ‹ä¸‹ä¸€ä¸ªtokenåŠå…¶æ¦‚ç‡
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            top_k: è¿”å›top-kä¸ªæœ€å¯èƒ½çš„token
            
        Returns:
            tokenå’Œæ¦‚ç‡çš„åˆ—è¡¨
        """
        # ç¼–ç è¾“å…¥æ–‡æœ¬
        inputs = self.tokenizer.encode(text, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            # è·å–æ¨¡å‹è¾“å‡º
            outputs = self.model(inputs)
            logits = outputs.logits[0, -1, :]  # æœ€åä¸€ä¸ªpositionçš„logits
            
            # åº”ç”¨softmaxè·å–æ¦‚ç‡
            probs = F.softmax(logits, dim=-1)
            
            # è·å–top-k
            top_probs, top_indices = torch.topk(probs, top_k)
            
            # è§£ç token
            results = []
            for prob, idx in zip(top_probs.cpu().numpy(), top_indices.cpu().numpy()):
                token = self.tokenizer.decode([idx])
                results.append((token, float(prob)))
            
            return results
    
    def generate_text(self, prompt: str, max_length: int = 100, 
                     temperature: float = 0.7, top_p: float = 0.9,
                     num_return_sequences: int = 1) -> List[str]:
        """
        ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompt: è¾“å…¥æç¤º
            max_length: æœ€å¤§é•¿åº¦
            temperature: æ¸©åº¦å‚æ•°ï¼ˆæ§åˆ¶éšæœºæ€§ï¼‰
            top_p: nucleus samplingå‚æ•°
            num_return_sequences: ç”Ÿæˆåºåˆ—æ•°é‡
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
        """
        try:
            # è®¾ç½®ç”Ÿæˆå‚æ•°
            generation_params = {
                "max_length": max_length,
                "temperature": temperature,
                "top_p": top_p,
                "num_return_sequences": num_return_sequences,
                "do_sample": True,
                "pad_token_id": self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
                "return_full_text": False
            }
            
            # ç”Ÿæˆæ–‡æœ¬
            outputs = self.generator(prompt, **generation_params)
            
            # æå–ç”Ÿæˆçš„æ–‡æœ¬
            generated_texts = []
            for output in outputs:
                generated_text = output['generated_text']
                generated_texts.append(generated_text)
            
            return generated_texts
            
        except Exception as e:
            print(f"âŒ æ–‡æœ¬ç”Ÿæˆå¤±è´¥: {e}")
            return [f"ç”Ÿæˆå¤±è´¥: {str(e)}"]
    
    def analyze_generation_process(self, text: str, num_steps: int = 10) -> Dict:
        """
        åˆ†æç”Ÿæˆè¿‡ç¨‹ä¸­çš„tokené¢„æµ‹
        
        Args:
            text: èµ·å§‹æ–‡æœ¬
            num_steps: åˆ†ææ­¥æ•°
            
        Returns:
            ç”Ÿæˆè¿‡ç¨‹åˆ†æç»“æœ
        """
        analysis_results = {
            "steps": [],
            "tokens": [],
            "probabilities": [],
            "generated_text": text
        }
        
        current_text = text
        
        for step in range(num_steps):
            # é¢„æµ‹ä¸‹ä¸€ä¸ªtoken
            next_tokens = self.predict_next_token(current_text, top_k=5)
            
            # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„token
            best_token, best_prob = next_tokens[0]
            
            # è®°å½•ä¿¡æ¯
            analysis_results["steps"].append(step + 1)
            analysis_results["tokens"].append(best_token)
            analysis_results["probabilities"].append(best_prob)
            
            # æ›´æ–°æ–‡æœ¬
            current_text += best_token
            analysis_results["generated_text"] = current_text
            
            print(f"æ­¥éª¤ {step + 1}: '{best_token}' (æ¦‚ç‡: {best_prob:.4f})")
        
        return analysis_results
    
    def save_model_checkpoint(self, save_path: str):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
        os.makedirs(save_path, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹å’Œåˆ†è¯å™¨
        self.model.save_pretrained(save_path)
        self.tokenizer.save_pretrained(save_path)
        
        # ä¿å­˜é…ç½®ä¿¡æ¯
        config = {
            "model_name": self.model_name,
            "device": self.device,
            "model_type": "causal_lm"
        }
        
        with open(os.path.join(save_path, "config.json"), "w", encoding="utf-8") as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")

class TextGenerationExperiment:
    """æ–‡æœ¬ç”Ÿæˆå®éªŒç±»"""
    
    def __init__(self, data_file: str):
        """
        åˆå§‹åŒ–å®éªŒ
        
        Args:
            data_file: æ•°æ®æ–‡ä»¶è·¯å¾„
        """
        self.data_file = data_file
        self.data = self._load_data()
        self.predictor = None
        self.results = {}
    
    def _load_data(self) -> Dict:
        """åŠ è½½å®éªŒæ•°æ®"""
        try:
            with open(self.data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(data.get('prompts', []))} ä¸ªæç¤º")
            return data
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            raise
    
    def setup_model(self, model_name: str = "facebook/opt-350m"):
        """è®¾ç½®æ¨¡å‹"""
        print(f"\nğŸ”§ è®¾ç½®æ¨¡å‹: {model_name}")
        self.predictor = NextTokenPredictor(model_name)
    
    def run_generation_experiments(self) -> Dict:
        """è¿è¡Œæ–‡æœ¬ç”Ÿæˆå®éªŒ"""
        if not self.predictor:
            raise ValueError("è¯·å…ˆè®¾ç½®æ¨¡å‹")
        
        print("\nğŸ§ª å¼€å§‹æ–‡æœ¬ç”Ÿæˆå®éªŒ...")
        
        results = {
            "prompts": [],
            "generated_texts": [],
            "generation_params": [],
            "analysis": []
        }
        
        # ä¸åŒçš„ç”Ÿæˆå‚æ•°è®¾ç½®
        param_sets = [
            {"temperature": 0.3, "top_p": 0.9, "name": "ä¿å®ˆç”Ÿæˆ"},
            {"temperature": 0.7, "top_p": 0.9, "name": "å¹³è¡¡ç”Ÿæˆ"},
            {"temperature": 1.0, "top_p": 0.8, "name": "åˆ›æ„ç”Ÿæˆ"}
        ]
        
        prompts = self.data.get("prompts", [])[:5]  # å–å‰5ä¸ªæç¤ºè¿›è¡Œå®éªŒ
        
        for i, prompt in enumerate(tqdm(prompts, desc="ç”Ÿæˆæ–‡æœ¬")):
            prompt_results = {
                "prompt": prompt,
                "generations": {}
            }
            
            for params in param_sets:
                # ç”Ÿæˆæ–‡æœ¬
                generated = self.predictor.generate_text(
                    prompt=prompt,
                    max_length=len(prompt.split()) + 30,  # åŠ¨æ€è®¾ç½®é•¿åº¦
                    temperature=params["temperature"],
                    top_p=params["top_p"],
                    num_return_sequences=1
                )
                
                prompt_results["generations"][params["name"]] = {
                    "text": generated[0] if generated else "",
                    "params": params
                }
            
            results["prompts"].append(prompt)
            results["generated_texts"].append(prompt_results)
        
        self.results["generation"] = results
        return results
    
    def run_next_token_analysis(self) -> Dict:
        """è¿è¡ŒNext Tokené¢„æµ‹åˆ†æ"""
        if not self.predictor:
            raise ValueError("è¯·å…ˆè®¾ç½®æ¨¡å‹")
        
        print("\nğŸ” å¼€å§‹Next Tokené¢„æµ‹åˆ†æ...")
        
        analysis_results = {
            "token_predictions": [],
            "generation_processes": []
        }
        
        # é€‰æ‹©å‡ ä¸ªä¸Šä¸‹æ–‡è¿›è¡Œåˆ†æ
        contexts = self.data.get("contexts", [])[:3]
        
        for context in tqdm(contexts, desc="åˆ†æé¢„æµ‹"):
            # é¢„æµ‹ä¸‹ä¸€ä¸ªtoken
            next_tokens = self.predictor.predict_next_token(context, top_k=10)
            
            analysis_results["token_predictions"].append({
                "context": context,
                "predictions": next_tokens
            })
            
            # åˆ†æç”Ÿæˆè¿‡ç¨‹
            generation_process = self.predictor.analyze_generation_process(
                context[:50] + "...",  # ä½¿ç”¨contextçš„å‰50ä¸ªå­—ç¬¦
                num_steps=5
            )
            
            analysis_results["generation_processes"].append({
                "initial_text": context[:50] + "...",
                "process": generation_process
            })
        
        self.results["analysis"] = analysis_results
        return analysis_results
    
    def visualize_results(self, save_dir: str):
        """å¯è§†åŒ–å®éªŒç»“æœ"""
        os.makedirs(save_dir, exist_ok=True)
        
        # 1. å¯è§†åŒ–tokené¢„æµ‹æ¦‚ç‡
        self._plot_token_predictions(save_dir)
        
        # 2. å¯è§†åŒ–ç”Ÿæˆè¿‡ç¨‹
        self._plot_generation_process(save_dir)
        
        # 3. ç”Ÿæˆå‚æ•°å¯¹æ¯”
        self._plot_parameter_comparison(save_dir)
    
    def _plot_token_predictions(self, save_dir: str):
        """ç»˜åˆ¶tokené¢„æµ‹æ¦‚ç‡å›¾"""
        if "analysis" not in self.results:
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        axes = axes.flatten()
        
        predictions = self.results["analysis"]["token_predictions"]
        
        for i, pred_data in enumerate(predictions[:4]):
            if i >= 4:
                break
                
            tokens = [item[0] for item in pred_data["predictions"]]
            probs = [item[1] for item in pred_data["predictions"]]
            
            # æ¸…ç†tokenæ˜¾ç¤º
            clean_tokens = [token.replace('\n', '\\n').replace(' ', '_') for token in tokens]
            
            axes[i].bar(range(len(clean_tokens)), probs, color='skyblue', alpha=0.7)
            axes[i].set_xlabel('Tokenæ’å')
            axes[i].set_ylabel('é¢„æµ‹æ¦‚ç‡')
            axes[i].set_title(f'ä¸Šä¸‹æ–‡ {i+1} çš„Tokené¢„æµ‹')
            axes[i].set_xticks(range(len(clean_tokens)))
            axes[i].set_xticklabels(clean_tokens, rotation=45, ha='right')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for j, prob in enumerate(probs):
                axes[i].text(j, prob + 0.01, f'{prob:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'token_predictions.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        print("âœ… Tokené¢„æµ‹æ¦‚ç‡å›¾å·²ä¿å­˜")
    
    def _plot_generation_process(self, save_dir: str):
        """ç»˜åˆ¶ç”Ÿæˆè¿‡ç¨‹å›¾"""
        if "analysis" not in self.results:
            return
        
        processes = self.results["analysis"]["generation_processes"]
        
        fig, axes = plt.subplots(len(processes), 1, figsize=(12, 4*len(processes)))
        if len(processes) == 1:
            axes = [axes]
        
        for i, process_data in enumerate(processes):
            process = process_data["process"]
            
            steps = process["steps"]
            probs = process["probabilities"]
            tokens = process["tokens"]
            
            # ç»˜åˆ¶æ¦‚ç‡å˜åŒ–
            axes[i].plot(steps, probs, marker='o', linewidth=2, markersize=6)
            axes[i].set_xlabel('ç”Ÿæˆæ­¥éª¤')
            axes[i].set_ylabel('é¢„æµ‹æ¦‚ç‡')
            axes[i].set_title(f'ç”Ÿæˆè¿‡ç¨‹ {i+1}: æ¦‚ç‡å˜åŒ–')
            axes[i].grid(True, alpha=0.3)
            
            # æ·»åŠ tokenæ ‡ç­¾
            for j, (step, prob, token) in enumerate(zip(steps, probs, tokens)):
                clean_token = token.replace('\n', '\\n').replace(' ', '_')
                axes[i].annotate(clean_token, (step, prob), 
                               textcoords="offset points", xytext=(0,10), ha='center')
        
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'generation_process.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        print("âœ… ç”Ÿæˆè¿‡ç¨‹å›¾å·²ä¿å­˜")
    
    def _plot_parameter_comparison(self, save_dir: str):
        """ç»˜åˆ¶å‚æ•°å¯¹æ¯”å›¾"""
        if "generation" not in self.results:
            return
        
        # ç»Ÿè®¡ä¸åŒå‚æ•°è®¾ç½®çš„æ–‡æœ¬é•¿åº¦
        param_stats = {}
        
        for prompt_result in self.results["generation"]["generated_texts"]:
            for param_name, gen_data in prompt_result["generations"].items():
                if param_name not in param_stats:
                    param_stats[param_name] = []
                
                text_length = len(gen_data["text"].split())
                param_stats[param_name].append(text_length)
        
        # ç»˜åˆ¶ç®±çº¿å›¾
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # æ–‡æœ¬é•¿åº¦å¯¹æ¯”
        param_names = list(param_stats.keys())
        lengths_data = [param_stats[name] for name in param_names]
        
        ax1.boxplot(lengths_data, labels=param_names)
        ax1.set_ylabel('ç”Ÿæˆæ–‡æœ¬é•¿åº¦ (è¯æ•°)')
        ax1.set_title('ä¸åŒå‚æ•°è®¾ç½®çš„æ–‡æœ¬é•¿åº¦å¯¹æ¯”')
        ax1.grid(True, alpha=0.3)
        
        # å‚æ•°è®¾ç½®å¯¹æ¯”
        temps = []
        top_ps = []
        names = []
        
        for prompt_result in self.results["generation"]["generated_texts"]:
            for param_name, gen_data in prompt_result["generations"].items():
                params = gen_data["params"]
                temps.append(params["temperature"])
                top_ps.append(params["top_p"])
                names.append(param_name)
        
        # åˆ›å»ºæ•£ç‚¹å›¾
        unique_names = list(set(names))
        colors = ['red', 'blue', 'green']
        
        for i, name in enumerate(unique_names):
            mask = [n == name for n in names]
            temp_vals = [t for t, m in zip(temps, mask) if m]
            top_p_vals = [p for p, m in zip(top_ps, mask) if m]
            
            ax2.scatter(temp_vals, top_p_vals, label=name, 
                       color=colors[i % len(colors)], s=100, alpha=0.7)
        
        ax2.set_xlabel('Temperature')
        ax2.set_ylabel('Top-p')
        ax2.set_title('ç”Ÿæˆå‚æ•°è®¾ç½®å¯¹æ¯”')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'parameter_comparison.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        print("âœ… å‚æ•°å¯¹æ¯”å›¾å·²ä¿å­˜")
    
    def save_results(self, save_dir: str):
        """ä¿å­˜å®éªŒç»“æœ"""
        os.makedirs(save_dir, exist_ok=True)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        with open(os.path.join(save_dir, 'experiment_results.json'), 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆå®éªŒæŠ¥å‘Š
        self._generate_report(save_dir)
        
        print(f"âœ… å®éªŒç»“æœå·²ä¿å­˜åˆ°: {save_dir}")
    
    def _generate_report(self, save_dir: str):
        """ç”Ÿæˆå®éªŒæŠ¥å‘Š"""
        report = []
        report.append("# Next Token Prediction å®éªŒæŠ¥å‘Š\n")
        report.append(f"å®éªŒæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # æ¨¡å‹ä¿¡æ¯
        if self.predictor:
            report.append("## æ¨¡å‹ä¿¡æ¯\n")
            report.append(f"- æ¨¡å‹åç§°: {self.predictor.model_name}\n")
            report.append(f"- è®¡ç®—è®¾å¤‡: {self.predictor.device}\n\n")
        
        # å®éªŒæ¦‚è¿°
        if "generation" in self.results:
            gen_results = self.results["generation"]
            report.append("## æ–‡æœ¬ç”Ÿæˆå®éªŒ\n")
            report.append(f"- æµ‹è¯•æç¤ºæ•°é‡: {len(gen_results['prompts'])}\n")
            report.append("- å‚æ•°è®¾ç½®: ä¿å®ˆç”Ÿæˆã€å¹³è¡¡ç”Ÿæˆã€åˆ›æ„ç”Ÿæˆ\n\n")
            
            # å±•ç¤ºéƒ¨åˆ†ç”Ÿæˆç»“æœ
            report.append("### ç”Ÿæˆç¤ºä¾‹\n")
            for i, prompt_result in enumerate(gen_results["generated_texts"][:2]):
                report.append(f"**æç¤º {i+1}**: {prompt_result['prompt']}\n\n")
                for param_name, gen_data in prompt_result["generations"].items():
                    report.append(f"- {param_name}: {gen_data['text'][:100]}...\n")
                report.append("\n")
        
        # Tokené¢„æµ‹åˆ†æ
        if "analysis" in self.results:
            analysis_results = self.results["analysis"]
            report.append("## Next Token é¢„æµ‹åˆ†æ\n")
            report.append(f"- åˆ†æä¸Šä¸‹æ–‡æ•°é‡: {len(analysis_results['token_predictions'])}\n")
            report.append(f"- ç”Ÿæˆè¿‡ç¨‹åˆ†æ: {len(analysis_results['generation_processes'])}\n\n")
        
        # æ ¸å¿ƒå‘ç°
        report.append("## æ ¸å¿ƒå‘ç°\n")
        report.append("1. **æ¸©åº¦å‚æ•°å½±å“**: ä½æ¸©åº¦ç”Ÿæˆæ›´ä¿å®ˆï¼Œé«˜æ¸©åº¦ç”Ÿæˆæ›´æœ‰åˆ›æ„\n")
        report.append("2. **Next Tokenæœºåˆ¶**: æ¨¡å‹åŸºäºä¸Šä¸‹æ–‡æ¦‚ç‡åˆ†å¸ƒé€‰æ‹©ä¸‹ä¸€ä¸ªè¯\n")
        report.append("3. **ç”Ÿæˆè´¨é‡**: æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆè¿è´¯çš„ä¸­æ–‡æ–‡æœ¬\n")
        report.append("4. **æ¦‚ç‡åˆ†å¸ƒ**: Top-ké¢„æµ‹æ˜¾ç¤ºäº†æ¨¡å‹çš„ä¸ç¡®å®šæ€§\n\n")
        
        report.append("## æ–‡ä»¶è¯´æ˜\n")
        report.append("- `experiment_results.json`: è¯¦ç»†å®éªŒæ•°æ®\n")
        report.append("- `token_predictions.png`: Tokené¢„æµ‹æ¦‚ç‡å¯è§†åŒ–\n")
        report.append("- `generation_process.png`: ç”Ÿæˆè¿‡ç¨‹åˆ†æ\n")
        report.append("- `parameter_comparison.png`: å‚æ•°è®¾ç½®å¯¹æ¯”\n")
        
        # å†™å…¥æŠ¥å‘Š
        with open(os.path.join(save_dir, 'experiment_report.md'), 'w', encoding='utf-8') as f:
            f.writelines(report)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ Next Token Prediction å®éªŒå¼€å§‹!")
    print("=" * 50)
    
    # è®¾ç½®éšæœºç§å­
    set_seed(42)
    
    # å®šä¹‰è·¯å¾„
    current_dir = os.path.dirname(os.path.abspath(__file__))
    data_file = os.path.join(current_dir, "text_generation_data.json")
    results_dir = os.path.join(current_dir, "results")
    checkpoint_dir = os.path.join(current_dir, "model-checkpoint")
    
    try:
        # 1. åˆå§‹åŒ–å®éªŒ
        experiment = TextGenerationExperiment(data_file)
        
        # 2. è®¾ç½®æ¨¡å‹ï¼ˆä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ä»¥ä¾¿å¿«é€Ÿå®éªŒï¼‰
        experiment.setup_model("facebook/opt-350m")
        
        # 3. è¿è¡Œæ–‡æœ¬ç”Ÿæˆå®éªŒ
        print("\nğŸ“ è¿è¡Œæ–‡æœ¬ç”Ÿæˆå®éªŒ...")
        generation_results = experiment.run_generation_experiments()
        
        # 4. è¿è¡ŒNext Tokenåˆ†æ
        print("\nğŸ” è¿è¡ŒNext Tokené¢„æµ‹åˆ†æ...")
        analysis_results = experiment.run_next_token_analysis()
        
        # 5. å¯è§†åŒ–ç»“æœ
        print("\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
        experiment.visualize_results(results_dir)
        
        # 6. ä¿å­˜ç»“æœ
        print("\nğŸ’¾ ä¿å­˜å®éªŒç»“æœ...")
        experiment.save_results(results_dir)
        
        # 7. ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
        print("\nğŸ”„ ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹...")
        experiment.predictor.save_model_checkpoint(checkpoint_dir)
        
        print("\n" + "=" * 50)
        print("ğŸ‰ Next Token Prediction å®éªŒå®Œæˆ!")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {results_dir}")
        print(f"ğŸ¤– æ¨¡å‹ä¿å­˜åœ¨: {checkpoint_dir}")
        
        # æ‰“å°ä¸€äº›å…³é”®ç»“æœ
        print("\nğŸ“‹ å®éªŒæ‘˜è¦:")
        if generation_results:
            print(f"âœ… å®Œæˆ {len(generation_results['prompts'])} ä¸ªæ–‡æœ¬ç”Ÿæˆå®éªŒ")
        if analysis_results:
            print(f"âœ… å®Œæˆ {len(analysis_results['token_predictions'])} ä¸ªTokené¢„æµ‹åˆ†æ")
        
    except Exception as e:
        print(f"\nâŒ å®éªŒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

# è¿è¡Œè¿‡ç¨‹ä¸­æç¤ºï¼š
# Both `max_new_tokens` (=256) and `max_length`(=32) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation )

# è¿™ä¸ªæŠ¥é”™æ˜¯ä»€ä¹ˆåŸå› 
# å®ƒçš„æ„æ€æ˜¯ï¼š
# ä½ åŒæ—¶ç»™æ¨¡å‹ä¼ äº† max_new_tokens=256 å’Œ max_length=32 ä¸¤ä¸ªå‚æ•°ï¼Œä½†è¿™ä¸¤ä¸ªå‚æ•°åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶åªèƒ½æœ‰ä¸€ä¸ªç”Ÿæ•ˆã€‚
# å½“å‰ max_new_tokens ä¼˜å…ˆçº§æ›´é«˜ï¼Œæ‰€ä»¥ çœŸæ­£ç”Ÿæ•ˆçš„æ˜¯ 256 ä¸ªæ–° tokenï¼Œè€Œ max_length=32 è¢«å¿½ç•¥äº†ã€‚
