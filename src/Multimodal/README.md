# å¤šæ¨¡æ€CLIPæ¨¡å‹è®­ç»ƒå®éªŒ

## é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®å®ç°äº†åŸºäºCLIPï¼ˆContrastive Language-Image Pre-trainingï¼‰çš„å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œä¸“æ³¨äºå›¾åƒ-æ–‡æœ¬è·¨æ¨¡æ€è¡¨ç¤ºå­¦ä¹ ã€‚é€šè¿‡å¯¹æ¯”å­¦ä¹ çš„æ–¹å¼ï¼Œä½¿å›¾åƒç¼–ç å™¨å’Œæ–‡æœ¬ç¼–ç å™¨å­¦ä¼šå°†ç›¸å…³çš„å›¾åƒå’Œæ–‡æœ¬æ˜ å°„åˆ°ç›¸åŒçš„å‘é‡ç©ºé—´ä¸­ï¼Œä»è€Œå®ç°é«˜è´¨é‡çš„è·¨æ¨¡æ€ç†è§£å’Œæ£€ç´¢ã€‚

## æ¨¡å‹åŸç†

### CLIPæ ¸å¿ƒæ€æƒ³

CLIPï¼ˆContrastive Language-Image Pre-trainingï¼‰æ˜¯OpenAIæå‡ºçš„å¤šæ¨¡æ€é¢„è®­ç»ƒæ¨¡å‹ï¼Œå…¶æ ¸å¿ƒæ€æƒ³åŒ…æ‹¬ï¼š

1. **è·¨æ¨¡æ€å¯¹é½**ï¼šå°†å›¾åƒå’Œæ–‡æœ¬ç¼–ç åˆ°åŒä¸€ä¸ªå‘é‡ç©ºé—´
2. **å¯¹æ¯”å­¦ä¹ **ï¼šé€šè¿‡å¯¹æ¯”æŸå¤±å‡½æ•°å­¦ä¹ ç›¸å…³å›¾åƒ-æ–‡æœ¬å¯¹çš„é«˜ç›¸ä¼¼åº¦
3. **é›¶æ ·æœ¬èƒ½åŠ›**ï¼šæ— éœ€é¢å¤–è®­ç»ƒå³å¯å¤„ç†æ–°çš„è§†è§‰æ¦‚å¿µ

### æ¨¡å‹æ¶æ„

```
è¾“å…¥å›¾åƒ â†’ å›¾åƒç¼–ç å™¨ â†’ å›¾åƒç‰¹å¾å‘é‡
è¾“å…¥æ–‡æœ¬ â†’ æ–‡æœ¬ç¼–ç å™¨ â†’ æ–‡æœ¬ç‰¹å¾å‘é‡
                    â†“
                å¯¹æ¯”å­¦ä¹ æŸå¤±
```

#### 1. å›¾åƒç¼–ç å™¨
- **æ¶æ„é€‰æ‹©**ï¼šæ”¯æŒVision Transformer (ViT)å’ŒResNet
- **é¢„å¤„ç†**ï¼šå›¾åƒresizeåˆ°224Ã—224ï¼Œæ ‡å‡†åŒ–
- **ç‰¹å¾æå–**ï¼šè¾“å‡ºé«˜ç»´å›¾åƒè¡¨ç¤ºå‘é‡
- **æŠ•å½±å±‚**ï¼šæ˜ å°„åˆ°ç»Ÿä¸€çš„ç‰¹å¾ç©ºé—´

#### 2. æ–‡æœ¬ç¼–ç å™¨
- **æ¶æ„**ï¼šåŸºäºTransformerçš„æ–‡æœ¬ç¼–ç å™¨
- **è¾“å…¥å¤„ç†**ï¼šæ–‡æœ¬tokenizationï¼Œæœ€å¤§é•¿åº¦77
- **ä¸Šä¸‹æ–‡å»ºæ¨¡**ï¼šæ•è·æ–‡æœ¬çš„è¯­ä¹‰ä¿¡æ¯
- **ç‰¹å¾æå–**ï¼šè¾“å‡ºæ–‡æœ¬è¡¨ç¤ºå‘é‡

#### 3. å¯¹æ¯”å­¦ä¹ æŸå¤±

å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°æ˜¯CLIPçš„æ ¸å¿ƒï¼ŒåŒ…å«ä¸¤ä¸ªæ–¹å‘ï¼š

**å›¾åƒåˆ°æ–‡æœ¬æŸå¤±**ï¼š
```
L_i2t = -log(exp(sim(img_i, txt_i) / Ï„) / Î£_j exp(sim(img_i, txt_j) / Ï„))
```

**æ–‡æœ¬åˆ°å›¾åƒæŸå¤±**ï¼š
```
L_t2i = -log(exp(sim(txt_i, img_i) / Ï„) / Î£_j exp(sim(txt_i, img_j) / Ï„))
```

**æ€»æŸå¤±**ï¼š
```
L_total = (L_i2t + L_t2i) / 2
```

å…¶ä¸­ï¼š
- `sim(a, b)` æ˜¯ä½™å¼¦ç›¸ä¼¼åº¦
- `Ï„` æ˜¯æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶è½¯æœ€å¤§å€¼çš„é”åº¦
- `i` æ˜¯åŒ¹é…çš„å›¾åƒ-æ–‡æœ¬å¯¹ï¼Œ`j` æ˜¯æ‰¹æ¬¡ä¸­çš„æ‰€æœ‰æ ·æœ¬

### è®­ç»ƒè¿‡ç¨‹

1. **æ•°æ®åŠ è½½**ï¼šæ‰¹é‡åŠ è½½å›¾åƒ-æ–‡æœ¬å¯¹
2. **ç‰¹å¾æå–**ï¼šåˆ†åˆ«ç¼–ç å›¾åƒå’Œæ–‡æœ¬
3. **ç›¸ä¼¼åº¦è®¡ç®—**ï¼šè®¡ç®—æ‰¹æ¬¡å†…æ‰€æœ‰å›¾åƒ-æ–‡æœ¬å¯¹çš„ç›¸ä¼¼åº¦
4. **æŸå¤±è®¡ç®—**ï¼šä½¿ç”¨å¯¹æ¯”æŸå¤±å‡½æ•°
5. **åå‘ä¼ æ’­**ï¼šæ›´æ–°æ¨¡å‹å‚æ•°
6. **è¯„ä¼°**ï¼šå®šæœŸåœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ£€ç´¢æ€§èƒ½

## é¡¹ç›®ç»“æ„

```
Multimodal/
â”œâ”€â”€ README.md                 # é¡¹ç›®è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ requirements.txt          # ä¾èµ–åŒ…åˆ—è¡¨
â”œâ”€â”€ config.json              # è®­ç»ƒé…ç½®æ–‡ä»¶
â”œâ”€â”€ setup.sh                 # ç¯å¢ƒå®‰è£…è„šæœ¬
â”œâ”€â”€ data_loader.py           # æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
â”œâ”€â”€ clip_model.py            # CLIPæ¨¡å‹å®šä¹‰
â”œâ”€â”€ train.py                 # è®­ç»ƒä¸»ç¨‹åº
â”œâ”€â”€ evaluation.py            # æ¨¡å‹è¯„ä¼°æ¨¡å—
â”œâ”€â”€ visualization.py         # ç»“æœå¯è§†åŒ–æ¨¡å—
â”œâ”€â”€ data/                    # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ images/             # å›¾åƒæ–‡ä»¶
â”‚   â”œâ”€â”€ captions/           # æ–‡æœ¬æè¿°æ–‡ä»¶
â”‚   â””â”€â”€ data.json           # æ•°æ®ç´¢å¼•æ–‡ä»¶
â”œâ”€â”€ model-checkpoint/        # æ¨¡å‹æ£€æŸ¥ç‚¹
â”œâ”€â”€ results/                # å®éªŒç»“æœ
â””â”€â”€ logs/                   # è®­ç»ƒæ—¥å¿—
```

## ä»£ç æ ¸å¿ƒæµç¨‹

### 1. æ•°æ®å¤„ç†æµç¨‹ (`data_loader.py`)

```python
# æ•°æ®åŠ è½½æµç¨‹
ImageTextDataset â†’ å›¾åƒé¢„å¤„ç† â†’ æ–‡æœ¬tokenization â†’ DataLoader
                     â†“
              æ‰¹é‡æ•°æ®å‡†å¤‡ â†’ è®­ç»ƒ/éªŒè¯åˆ†å‰²
```

**å…³é”®åŠŸèƒ½**ï¼š
- æ”¯æŒå¤šç§æ•°æ®æ ¼å¼ï¼ˆJSONã€ç›®å½•ç»“æ„ï¼‰
- å›¾åƒé¢„å¤„ç†ï¼ˆresizeã€normalizeï¼‰
- æ–‡æœ¬tokenizationï¼ˆä½¿ç”¨CLIP tokenizerï¼‰
- æ•°æ®å¢å¼ºï¼ˆå¯é€‰ï¼‰

### 2. æ¨¡å‹å®šä¹‰æµç¨‹ (`clip_model.py`)

```python
# æ¨¡å‹åˆå§‹åŒ–æµç¨‹
é¢„è®­ç»ƒCLIPæ¨¡å‹ â†’ æ·»åŠ æŠ•å½±å±‚ â†’ è®¾ç½®è®­ç»ƒå‚æ•°
                     â†“
              å‰å‘ä¼ æ’­ â†’ ç‰¹å¾æå– â†’ å¯¹æ¯”å­¦ä¹ 
```

**æ ¸å¿ƒç»„ä»¶**ï¼š
- `CLIPModel`: ä¸»æ¨¡å‹ç±»ï¼Œå°è£…å›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨
- `ContrastiveLoss`: å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°
- `MultimodalTrainer`: è®­ç»ƒå™¨ï¼Œç®¡ç†è®­ç»ƒè¿‡ç¨‹

### 3. è®­ç»ƒæµç¨‹ (`train.py`)

```python
# å®Œæ•´è®­ç»ƒæµç¨‹
é…ç½®åˆå§‹åŒ– â†’ æ•°æ®å‡†å¤‡ â†’ æ¨¡å‹åˆ›å»º â†’ è®­ç»ƒå¾ªç¯ â†’ æ¨¡å‹è¯„ä¼° â†’ ç»“æœä¿å­˜
```

**è®­ç»ƒæ­¥éª¤**ï¼š
1. ç¯å¢ƒè®¾ç½®å’Œé…ç½®åŠ è½½
2. æ•°æ®åŠ è½½å™¨åˆ›å»º
3. æ¨¡å‹å’Œä¼˜åŒ–å™¨åˆå§‹åŒ–
4. è®­ç»ƒå¾ªç¯ï¼ˆå‰å‘ä¼ æ’­â†’æŸå¤±è®¡ç®—â†’åå‘ä¼ æ’­ï¼‰
5. éªŒè¯å’Œæ—©åœæ£€æŸ¥
6. æ¨¡å‹ä¿å­˜å’Œç»“æœå¯è§†åŒ–

### 4. è¯„ä¼°æµç¨‹ (`evaluation.py`)

```python
# è¯„ä¼°æŒ‡æ ‡è®¡ç®—
ç‰¹å¾æå– â†’ ç›¸ä¼¼åº¦çŸ©é˜µ â†’ æ£€ç´¢æ’å â†’ æ€§èƒ½æŒ‡æ ‡è®¡ç®—
```

**è¯„ä¼°æŒ‡æ ‡**ï¼š
- **Recall@K**: Top-Kæ£€ç´¢å‡†ç¡®ç‡
- **Mean Rank**: å¹³å‡æ’å
- **RSum**: æ‰€æœ‰RecallæŒ‡æ ‡çš„æ€»å’Œ
- **ç›¸ä¼¼åº¦åˆ†æ**: æ­£è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦åˆ†å¸ƒ

## ç¯å¢ƒå®‰è£…

### è‡ªåŠ¨å®‰è£…ï¼ˆæ¨èï¼‰

```bash
# ç»™å®‰è£…è„šæœ¬æ‰§è¡Œæƒé™
chmod +x setup.sh

# è¿è¡Œå®‰è£…è„šæœ¬
./setup.sh
```

### æ‰‹åŠ¨å®‰è£…

```bash
# 1. å®‰è£…PyTorchï¼ˆæ ¹æ®æ‚¨çš„CUDAç‰ˆæœ¬é€‰æ‹©ï¼‰
# CPUç‰ˆæœ¬
pip install torch torchvision torchaudio

# CUDAç‰ˆæœ¬ï¼ˆä»¥CUDA 11.8ä¸ºä¾‹ï¼‰
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# 2. å®‰è£…CLIP
pip install git+https://github.com/openai/CLIP.git

# 3. å®‰è£…å…¶ä»–ä¾èµ–
pip install -r requirements.txt
```

### ç¯å¢ƒè¦æ±‚

- Python >= 3.8
- PyTorch >= 1.12.0
- CUDA >= 11.0 (GPUè®­ç»ƒ)
- è‡³å°‘8GBå†…å­˜ï¼ˆCPUï¼‰æˆ–4GBæ˜¾å­˜ï¼ˆGPUï¼‰

## æ•°æ®å‡†å¤‡

### æ•°æ®æ ¼å¼

æ”¯æŒä¸¤ç§æ•°æ®æ ¼å¼ï¼š

#### æ ¼å¼1ï¼šJSONç´¢å¼•æ–‡ä»¶
```json
[
  {
    "image_path": "path/to/image1.jpg",
    "caption": "A beautiful sunset over the ocean"
  },
  {
    "image_path": "path/to/image2.jpg", 
    "caption": "A cat sitting on a window sill"
  }
]
```

#### æ ¼å¼2ï¼šç›®å½•ç»“æ„
```
data/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ image001.jpg
â”‚   â”œâ”€â”€ image002.jpg
â”‚   â””â”€â”€ ...
â””â”€â”€ captions/
    â”œâ”€â”€ image001.txt
    â”œâ”€â”€ image002.txt
    â””â”€â”€ ...
```

### æ•°æ®è¦æ±‚

- **å›¾åƒæ ¼å¼**ï¼šJPGã€PNGç­‰å¸¸è§æ ¼å¼
- **å›¾åƒå°ºå¯¸**ï¼šä»»æ„å°ºå¯¸ï¼ˆä¼šè‡ªåŠ¨resizeåˆ°224Ã—224ï¼‰
- **æ–‡æœ¬é•¿åº¦**ï¼šå»ºè®®ä¸è¶…è¿‡77ä¸ªtoken
- **æ•°æ®é‡**ï¼šæœ€å°‘50å¯¹ï¼Œæ¨è1000+å¯¹ç”¨äºæœ‰æ•ˆè®­ç»ƒ

### åˆ›å»ºç¤ºä¾‹æ•°æ®

å¦‚æœæ²¡æœ‰å‡†å¤‡æ•°æ®ï¼Œç¨‹åºä¼šè‡ªåŠ¨åˆ›å»ºç¤ºä¾‹æ•°æ®ï¼š

```python
from data_loader import create_sample_dataset

# åˆ›å»º100ä¸ªç¤ºä¾‹æ ·æœ¬
create_sample_dataset("./data", num_samples=100)
```

## ä½¿ç”¨æ–¹æ³•

### åŸºç¡€è®­ç»ƒ

```bash
# ä½¿ç”¨é»˜è®¤å‚æ•°è®­ç»ƒ
python train.py

# è‡ªå®šä¹‰å‚æ•°è®­ç»ƒ
python train.py --batch_size 64 --num_epochs 100 --learning_rate 5e-5
```

### å‚æ•°è¯´æ˜

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--data_dir` | "./data" | æ•°æ®ç›®å½•è·¯å¾„ |
| `--model_name` | "ViT-B/32" | é¢„è®­ç»ƒæ¨¡å‹åç§° |
| `--batch_size` | 32 | æ‰¹æ¬¡å¤§å° |
| `--num_epochs` | 50 | è®­ç»ƒè½®æ•° |
| `--learning_rate` | 1e-4 | å­¦ä¹ ç‡ |
| `--freeze_backbone` | False | æ˜¯å¦å†»ç»“ä¸»å¹²ç½‘ç»œ |
| `--device` | auto | è®¡ç®—è®¾å¤‡ |

### å¯ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹

- `ViT-B/32`: Vision Transformer Base (æ¨è)
- `ViT-B/16`: Vision Transformer Base é«˜åˆ†è¾¨ç‡
- `ViT-L/14`: Vision Transformer Large 
- `RN50`: ResNet-50
- `RN101`: ResNet-101

### é«˜çº§é…ç½®

ç¼–è¾‘ `config.json` æ–‡ä»¶è¿›è¡Œé«˜çº§é…ç½®ï¼š

```json
{
  "model": {
    "name": "ViT-B/32",
    "projection_dim": 512,
    "freeze_backbone": false
  },
  "training": {
    "batch_size": 32,
    "learning_rate": 1e-4,
    "weight_decay": 0.01,
    "gradient_clip_norm": 1.0
  }
}
```

## ç»“æœåˆ†æ

### è®­ç»ƒç»“æœ

è®­ç»ƒå®Œæˆåï¼Œåœ¨`results/`ç›®å½•ä¸‹ä¼šç”Ÿæˆï¼š

1. **è®­ç»ƒæ›²çº¿å›¾** (`training_curves.png`)
   - æŸå¤±å˜åŒ–æ›²çº¿
   - å‡†ç¡®ç‡å˜åŒ–æ›²çº¿
   - å­¦ä¹ ç‡å˜åŒ–æ›²çº¿

2. **æ¨¡å‹æ€§èƒ½å›¾** (`model_metrics.png`)
   - æ£€ç´¢æ€§èƒ½æ¡å½¢å›¾
   - ç›¸ä¼¼åº¦åˆ†æå›¾

3. **è®­ç»ƒæ€»ç»“** (`training_results.json`)
   - æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡
   - è®­ç»ƒé…ç½®
   - è®­ç»ƒå†å²

### æ€§èƒ½æŒ‡æ ‡è§£è¯»

#### æ£€ç´¢æŒ‡æ ‡
- **R@1**: Top-1æ£€ç´¢å‡†ç¡®ç‡ï¼Œè¶Šé«˜è¶Šå¥½
- **R@5**: Top-5æ£€ç´¢å‡†ç¡®ç‡ï¼Œè¶Šé«˜è¶Šå¥½
- **R@10**: Top-10æ£€ç´¢å‡†ç¡®ç‡ï¼Œè¶Šé«˜è¶Šå¥½
- **RSum**: æ‰€æœ‰æ£€ç´¢æŒ‡æ ‡çš„æ€»å’Œï¼Œç»¼åˆæ€§èƒ½æŒ‡æ ‡

#### ç›¸ä¼¼åº¦æŒ‡æ ‡
- **Positive Similarity**: åŒ¹é…å¯¹çš„å¹³å‡ç›¸ä¼¼åº¦
- **Negative Similarity**: éåŒ¹é…å¯¹çš„å¹³å‡ç›¸ä¼¼åº¦
- **Similarity Gap**: æ­£è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦å·®è·ï¼Œè¶Šå¤§è¶Šå¥½

### æ€§èƒ½åŸºå‡†

åœ¨æ ‡å‡†æ•°æ®é›†ä¸Šçš„å…¸å‹æ€§èƒ½ï¼š

| æ¨¡å‹ | æ•°æ®é›†å¤§å° | I2T R@1 | T2I R@1 | RSum |
|------|------------|---------|---------|------|
| ViT-B/32 | 1K | 30-50% | 25-45% | 200-300 |
| ViT-B/32 | 10K | 40-60% | 35-55% | 250-350 |
| ViT-B/16 | 10K | 45-65% | 40-60% | 300-400 |

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. CUDAå†…å­˜ä¸è¶³
```
RuntimeError: CUDA out of memory
```
**è§£å†³æ–¹æ¡ˆ**ï¼š
- å‡å°`batch_size`
- ä½¿ç”¨`freeze_backbone=True`
- ä½¿ç”¨CPUè®­ç»ƒ

#### 2. æ•°æ®åŠ è½½é”™è¯¯
```
FileNotFoundError: No such file or directory
```
**è§£å†³æ–¹æ¡ˆ**ï¼š
- æ£€æŸ¥æ•°æ®è·¯å¾„æ˜¯å¦æ­£ç¡®
- ç¡®ä¿å›¾åƒæ–‡ä»¶å­˜åœ¨
- è¿è¡Œæ•°æ®åˆ›å»ºè„šæœ¬

#### 3. æ¨¡å‹ä¸‹è½½å¤±è´¥
```
urllib.error.URLError
```
**è§£å†³æ–¹æ¡ˆ**ï¼š
- æ£€æŸ¥ç½‘ç»œè¿æ¥
- ä½¿ç”¨ä»£ç†æˆ–é•œåƒæº
- æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶

#### 4. æ€§èƒ½ä¸ä½³
å¦‚æœæ¨¡å‹æ€§èƒ½è¾ƒå·®ï¼š
- å¢åŠ è®­ç»ƒæ•°æ®é‡
- è°ƒæ•´å­¦ä¹ ç‡ï¼ˆå°è¯•1e-5åˆ°1e-3ï¼‰
- å¢åŠ è®­ç»ƒè½®æ•°
- ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ï¼ˆViT-B/16æˆ–ViT-L/14ï¼‰

### è°ƒè¯•æ¨¡å¼

å¯ç”¨è¯¦ç»†æ—¥å¿—ï¼š

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

### æ€§èƒ½ä¼˜åŒ–

1. **æ•°æ®åŠ è½½ä¼˜åŒ–**ï¼š
   - å¢åŠ `num_workers`
   - å¯ç”¨`pin_memory`
   - ä½¿ç”¨SSDå­˜å‚¨æ•°æ®

2. **è®­ç»ƒä¼˜åŒ–**ï¼š
   - ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
   - æ¢¯åº¦ç´¯ç§¯
   - å­¦ä¹ ç‡è°ƒåº¦

3. **å†…å­˜ä¼˜åŒ–**ï¼š
   - æ¢¯åº¦æ£€æŸ¥ç‚¹
   - æ¨¡å‹å¹¶è¡Œ
   - æ•°æ®å¹¶è¡Œ

## æ‰©å±•åŠŸèƒ½

### è‡ªå®šä¹‰æ•°æ®å¢å¼º

```python
from torchvision import transforms

custom_transform = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                        std=[0.229, 0.224, 0.225])
])
```

### æ¨¡å‹æ¨ç†

```python
from clip_model import CLIPModel
import torch

# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
model = CLIPModel()
checkpoint = torch.load("model-checkpoint/best_model.pth")
model.load_state_dict(checkpoint['model_state_dict'])

# æ¨ç†
with torch.no_grad():
    image_features = model.encode_image(images)
    text_features = model.encode_text(texts)
    similarity = torch.matmul(image_features, text_features.T)
```

### é›†æˆåˆ°å…¶ä»–é¡¹ç›®

```python
# å¯¼å…¥æ¨¡å‹
from clip_model import CLIPModel

# åˆ›å»ºæ¨¡å‹å®ä¾‹
model = CLIPModel(model_name="ViT-B/32")

# åŠ è½½é¢„è®­ç»ƒæƒé‡
model.load_checkpoint("path/to/checkpoint.pth")

# ä½¿ç”¨æ¨¡å‹è¿›è¡Œç‰¹å¾æå–æˆ–ç›¸ä¼¼åº¦è®¡ç®—
```

## å‚è€ƒæ–‡çŒ®

1. Radford, A., et al. "Learning Transferable Visual Models From Natural Language Supervision." ICML 2021.
2. Dosovitskiy, A., et al. "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale." ICLR 2021.
3. Chen, T., et al. "A Simple Framework for Contrastive Learning of Visual Representations." ICML 2020.

## è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäºMITè®¸å¯è¯å¼€æºã€‚

## è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›é¡¹ç›®ï¼š

1. Forkæœ¬é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯
3. æäº¤æ›´æ”¹
4. æ¨é€åˆ°åˆ†æ”¯
5. åˆ›å»ºPull Request

## è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»ï¼š
- é¡¹ç›®Issuesé¡µé¢
- é‚®ä»¶ï¼šyour-email@example.com

---

**Happy Training! ğŸš€**
